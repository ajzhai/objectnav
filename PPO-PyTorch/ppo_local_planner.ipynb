{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "import argparse\n",
    "import quaternion\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.distributions import Categorical\n",
    "import habitat\n",
    "\n",
    "import gym\n",
    "from gym.spaces import Discrete, Box, Tuple\n",
    "from slam_agent import ORBSLAM2Agent, config\n",
    "\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Memory:\n",
    "    def __init__(self):\n",
    "        self.actions = []\n",
    "        self.map_states = []\n",
    "        self.coord_states = []\n",
    "        self.logprobs = []\n",
    "        self.rewards = []\n",
    "        self.is_terminals = []\n",
    "    \n",
    "    def clear_memory(self):\n",
    "        del self.actions[:]\n",
    "        del self.map_states[:]\n",
    "        del self.coord_states[:]\n",
    "        del self.logprobs[:]\n",
    "        del self.rewards[:]\n",
    "        del self.is_terminals[:]\n",
    "\n",
    "        \n",
    "class MyFeatureExtractor(nn.Module):\n",
    "    \n",
    "    def __init__(self, features_dim: int = 256):\n",
    "        super(MyFeatureExtractor, self).__init__()\n",
    "        self.cnn = nn.Sequential(nn.Conv2d(1, 32, kernel_size=5, stride=2, padding=0),\n",
    "                                 \n",
    "                                 nn.MaxPool2d(2),\n",
    "                                 nn.ReLU(),\n",
    "                                 nn.Conv2d(32, 64, kernel_size=3, stride=2, padding=0),\n",
    "                                 nn.MaxPool2d(2), \n",
    "                                 nn.ReLU(),\n",
    "                                 nn.Conv2d(64, 128, kernel_size=3, stride=2, padding=0),\n",
    "                                 nn.ReLU(),\n",
    "                                 nn.Conv2d(128, 256, kernel_size=3, stride=1, padding=0),\n",
    "                                 nn.ReLU(),\n",
    "                                 nn.AdaptiveAvgPool2d((1, 1)))  # b, 256, 1, 1\n",
    "        self.features_dim = features_dim\n",
    "        self.linear = nn.Sequential(nn.Linear(256 + 8, features_dim), nn.ReLU())\n",
    "    \n",
    "    def forward(self, coord_state, map_state):\n",
    "        map_features = self.cnn(map_state).reshape(-1, 256)\n",
    "        return self.linear(torch.cat([map_features, coord_state], dim=1))\n",
    "        \n",
    "        \n",
    "class ActorCritic(nn.Module):\n",
    "    def __init__(self, state_dim, action_dim, n_latent_var):\n",
    "        super(ActorCritic, self).__init__()\n",
    "\n",
    "        # shared layers\n",
    "        self.feature_layer = MyFeatureExtractor()\n",
    "        \n",
    "        # actor\n",
    "        self.action_layer = nn.Sequential(\n",
    "                nn.Linear(state_dim, n_latent_var),\n",
    "                nn.Tanh(),\n",
    "                nn.Linear(n_latent_var, n_latent_var),\n",
    "                nn.Tanh(),\n",
    "                nn.Linear(n_latent_var, action_dim),\n",
    "                nn.Softmax(dim=-1)\n",
    "                )\n",
    "        \n",
    "        # critic\n",
    "        self.value_layer = nn.Sequential(\n",
    "                nn.Linear(state_dim, n_latent_var),\n",
    "                nn.Tanh(),\n",
    "                nn.Linear(n_latent_var, n_latent_var),\n",
    "                nn.Tanh(),\n",
    "                nn.Linear(n_latent_var, 1)\n",
    "                )\n",
    "        \n",
    "    def forward(self):\n",
    "        raise NotImplementedError\n",
    "        \n",
    "    def act(self, state, memory):\n",
    "        agent_info, map_info, goal_info, latest_map = state\n",
    "        coord_state = np.concatenate([agent_info, map_info, goal_info])\n",
    "        coord_state = torch.from_numpy(coord_state).float().to(device).unsqueeze(0) \n",
    "        map_state = torch.from_numpy(latest_map).float().to(device).unsqueeze(0) \n",
    "        features = self.feature_layer(coord_state, map_state)\n",
    "        \n",
    "        action_probs = self.action_layer(features)\n",
    "        dist = Categorical(action_probs)\n",
    "        action = dist.sample()\n",
    "        \n",
    "        memory.coord_states.append(coord_state)\n",
    "        memory.map_states.append(map_state)\n",
    "        memory.actions.append(action)\n",
    "        memory.logprobs.append(dist.log_prob(action))\n",
    "        \n",
    "        return action.item()\n",
    "    \n",
    "    def evaluate(self, coord_state, map_state, action):\n",
    "        features = self.feature_layer(coord_state, map_state)\n",
    "        \n",
    "        action_probs = self.action_layer(features)\n",
    "        dist = Categorical(action_probs)\n",
    "        \n",
    "        action_logprobs = dist.log_prob(action)\n",
    "        dist_entropy = dist.entropy()\n",
    "        \n",
    "        state_value = self.value_layer(features)\n",
    "        \n",
    "        return action_logprobs, torch.squeeze(state_value), dist_entropy\n",
    "        \n",
    "class PPO:\n",
    "    def __init__(self, state_dim, action_dim, n_latent_var, lr, betas, gamma, K_epochs, eps_clip):\n",
    "        self.lr = lr\n",
    "        self.betas = betas\n",
    "        self.gamma = gamma\n",
    "        self.eps_clip = eps_clip\n",
    "        self.K_epochs = K_epochs\n",
    "        \n",
    "        self.policy = ActorCritic(state_dim, action_dim, n_latent_var).to(device)\n",
    "        self.optimizer = torch.optim.Adam(self.policy.parameters(), lr=lr, betas=betas)\n",
    "        self.policy_old = ActorCritic(state_dim, action_dim, n_latent_var).to(device)\n",
    "        self.policy_old.load_state_dict(self.policy.state_dict())\n",
    "        \n",
    "        self.MseLoss = nn.MSELoss()\n",
    "    \n",
    "    def update(self, memory):   \n",
    "        # Monte Carlo estimate of state rewards:\n",
    "        rewards = []\n",
    "        discounted_reward = 0\n",
    "        for reward, is_terminal in zip(reversed(memory.rewards), reversed(memory.is_terminals)):\n",
    "            if is_terminal:\n",
    "                discounted_reward = 0\n",
    "            discounted_reward = reward + (self.gamma * discounted_reward)\n",
    "            rewards.insert(0, discounted_reward)\n",
    "        \n",
    "        # Normalizing the rewards:\n",
    "        rewards = torch.tensor(rewards).to(device)\n",
    "        rewards = (rewards - rewards.mean()) / (rewards.std() + 1e-5)\n",
    "        \n",
    "        # convert list to tensor\n",
    "        old_coord_states = torch.cat(memory.coord_states, dim=0)\n",
    "        old_map_states = torch.cat(memory.map_states, dim=0)\n",
    "        #old_states = torch.stack(memory.states).to(device).detach()\n",
    "        old_actions = torch.stack(memory.actions).to(device).detach()\n",
    "        old_logprobs = torch.stack(memory.logprobs).to(device).detach()\n",
    "        \n",
    "        # Optimize policy for K epochs:\n",
    "        for _ in range(self.K_epochs):\n",
    "            # Evaluating old actions and values :\n",
    "            logprobs, state_values, dist_entropy = self.policy.evaluate(old_coord_states, \n",
    "                                                                        old_map_states, \n",
    "                                                                        old_actions)\n",
    "            \n",
    "            # Finding the ratio (pi_theta / pi_theta__old):\n",
    "            ratios = torch.exp(logprobs - old_logprobs.detach())\n",
    "                \n",
    "            # Finding Surrogate Loss:\n",
    "            advantages = rewards - state_values.detach()\n",
    "            surr1 = ratios * advantages\n",
    "            surr2 = torch.clamp(ratios, 1-self.eps_clip, 1+self.eps_clip) * advantages\n",
    "            loss = -torch.min(surr1, surr2) + 0.5*self.MseLoss(state_values, rewards) - 0.01*dist_entropy\n",
    "            \n",
    "            # take gradient step\n",
    "            self.optimizer.zero_grad()\n",
    "            loss.mean().backward()\n",
    "            self.optimizer.step()\n",
    "        \n",
    "        # Copy new weights into old policy:\n",
    "        self.policy_old.load_state_dict(self.policy.state_dict())\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "parser = argparse.ArgumentParser()\n",
    "parser.add_argument(\n",
    "    \"--agent-type\",\n",
    "    default=\"orbslam2-rgbd\",\n",
    "    choices=[\"blind\", \"orbslam2-rgbd\", \"orbslam2-rgb-monod\"],\n",
    ")\n",
    "parser.add_argument(\n",
    "    \"--task-config\", type=str, default=\"tasks/pointnav_rgbd.yaml\"\n",
    ")\n",
    "parser.add_argument(\n",
    "    \"--goal-sensor-uuid\", type=str, default=\"pointgoal_with_gps_compass\"\n",
    ")\n",
    "args = parser.parse_args([\n",
    "    \"--task-config\", \"../habitat-api/configs/tasks/objectnav_mp3d_fast.yaml\",\n",
    "    \"--goal-sensor-uuid\", \"objectgoal\"\n",
    "])\n",
    "\n",
    "global GOAL_SENSOR_UUID\n",
    "GOAL_SENSOR_UUID = args.goal_sensor_uuid\n",
    "\n",
    "agent = ORBSLAM2Agent(config.ORBSLAM2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2020-06-11 19:03:57,388 Initializing dataset ObjectNav-v1\n",
      "2020-06-11 19:03:57,500 initializing sim Sim-v0\n",
      "I0611 19:04:02.824032 20467 simulator.py:142] Loaded navmesh /home/azav/habitat-api/data/scene_datasets/mp3d/x8F5xyUWy9e/x8F5xyUWy9e.navmesh\n",
      "2020-06-11 19:04:02,828 Initializing task ObjectNav-v1\n"
     ]
    }
   ],
   "source": [
    "task_config = habitat.get_config(config_paths=args.task_config)\n",
    "env = habitat.Env(config=task_config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LocalNav(gym.Env):\n",
    "    \"\"\"\n",
    "    RL environment for low-level motion planning.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, hab_agent):\n",
    "        self.ha = hab_agent\n",
    "        self.action_space = Discrete(3)  # 0 = forward, 1 = turn left, 2 = turn right (habitat - 1)\n",
    "        self.latest_map = None\n",
    "        self.aloc = None\n",
    "        self.arot = None\n",
    "        self.random_goal = None  # in GPS coordinates\n",
    "        self.observation_space = Tuple((Box(low=np.array([-np.inf] * 3),  # agent loc and rot\n",
    "                                            high=np.array([-np.inf] * 3),\n",
    "                                            dtype=np.float32),\n",
    "                                        Box(low=np.array([-np.inf] * 3),  # map loc and rot\n",
    "                                            high=np.array([-np.inf] * 3),\n",
    "                                            dtype=np.float32),\n",
    "                                        Box(low=np.array([-np.inf, -np.inf]),  # goal loc\n",
    "                                            high=np.array([np.inf, np.inf]),\n",
    "                                            dtype=np.float32),\n",
    "                                        Box(low=0, high=1,  # SLAM obstacle map\n",
    "                                            shape=(1, 400, 400),\n",
    "                                            dtype=np.float32)))\n",
    "\n",
    "    def agent_observation(self):\n",
    "        \"\"\"RL agent's state observation.\"\"\"\n",
    "        return (np.append(self.aloc, self.arot), \n",
    "                self.ha.curr_map_info, \n",
    "                self.random_goal, \n",
    "                self.latest_map[np.newaxis])\n",
    "        \n",
    "    def reset(self):\n",
    "        \"\"\"Called at the end of each episode to reset position.\"\"\"\n",
    "        obs = env.reset()\n",
    "        self.ha.reset()\n",
    "        \n",
    "\n",
    "        # Random goal location\n",
    "        rtra = env._sim.sample_navigable_point()\n",
    "        rrot = quaternion.from_euler_angles(np.array([0, 0, 0]))\n",
    "        obs = env._sim.get_observations_at(position=rtra, rotation=rrot, keep_agent_at_new_pose=True)\n",
    "        obs = env.step(1)\n",
    "        self.random_goal = obs['gps']\n",
    "        \n",
    "        # Random starting location\n",
    "        rtra = env._sim.sample_navigable_point()\n",
    "        ang = np.random.rand() * 2 * np.pi\n",
    "        rrot = quaternion.from_euler_angles(np.array([0, ang, 0]))\n",
    "        obs = env._sim.get_observations_at(position=rtra, rotation=rrot, keep_agent_at_new_pose=True)\n",
    "        obs = env.step(1)\n",
    "        self.aloc, self.arot = obs['gps'], obs['compass']\n",
    "        self.ha.update_internal_state(obs)\n",
    "        \n",
    "        if 'numpy' in str(type(self.ha.map2DObstacles)): \n",
    "            self.latest_map = (self.ha.map2DObstacles > self.ha.obstacle_th).astype(np.uint8)\n",
    "        else:\n",
    "            self.latest_map = (self.ha.map2DObstacles[0,0].cpu().numpy() > self.ha.obstacle_th).astype(np.uint8)\n",
    "            \n",
    "        return self.agent_observation()\n",
    "\n",
    "    def step(self, action):\n",
    "        \"\"\"\n",
    "        MDP step.\n",
    "\n",
    "        :param action: 0 forward, 1 turnleft, 2 turnright\n",
    "        :return: state, reward, done (bool), auxiliary info\n",
    "        \"\"\"\n",
    "        \n",
    "        obs = env.step(action + 1)\n",
    "        self.ha.update_internal_state(obs)\n",
    "        self.aloc, self.arot = obs['gps'], obs['compass']\n",
    "        \n",
    "        if 'numpy' in str(type(self.ha.map2DObstacles)): \n",
    "            self.latest_map = (self.ha.map2DObstacles > self.ha.obstacle_th).astype(np.uint8)\n",
    "        else:\n",
    "            self.latest_map = (self.ha.map2DObstacles[0,0].cpu().numpy() > self.ha.obstacle_th).astype(np.uint8)\n",
    "            \n",
    "            \n",
    "        reward = -1  # time-minimizing reward\n",
    "        done = np.linalg.norm(self.aloc - self.random_goal) < 0.5  # reached goal\n",
    "        return self.agent_observation(), reward, done, {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.002 (0.9, 0.999)\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "ename": "RuntimeError",
     "evalue": "CUDA out of memory. Tried to allocate 154.00 MiB (GPU 0; 31.72 GiB total capacity; 4.13 GiB already allocated; 62.38 MiB free; 29.23 MiB cached)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-11-ba293e553853>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     48\u001b[0m         \u001b[0;31m# update if its time\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     49\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mtimestep\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0mupdate_timestep\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 50\u001b[0;31m             \u001b[0mppo\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmemory\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     51\u001b[0m             \u001b[0mmemory\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclear_memory\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     52\u001b[0m             \u001b[0mtimestep\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-7-815d64ffd413>\u001b[0m in \u001b[0;36mupdate\u001b[0;34m(self, memory)\u001b[0m\n\u001b[1;32m    142\u001b[0m             logprobs, state_values, dist_entropy = self.policy.evaluate(old_coord_states, \n\u001b[1;32m    143\u001b[0m                                                                         \u001b[0mold_map_states\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 144\u001b[0;31m                                                                         old_actions)\n\u001b[0m\u001b[1;32m    145\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    146\u001b[0m             \u001b[0;31m# Finding the ratio (pi_theta / pi_theta__old):\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-7-815d64ffd413>\u001b[0m in \u001b[0;36mevaluate\u001b[0;34m(self, coord_state, map_state, action)\u001b[0m\n\u001b[1;32m     89\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     90\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mevaluate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcoord_state\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmap_state\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maction\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 91\u001b[0;31m         \u001b[0mfeatures\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfeature_layer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcoord_state\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmap_state\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     92\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     93\u001b[0m         \u001b[0maction_probs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0maction_layer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfeatures\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/habitat/lib/python3.6/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    491\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    492\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 493\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    494\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    495\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-7-815d64ffd413>\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, coord_state, map_state)\u001b[0m\n\u001b[1;32m     37\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     38\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcoord_state\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmap_state\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 39\u001b[0;31m         \u001b[0mmap_features\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcnn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmap_state\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreshape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m256\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     40\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlinear\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mmap_features\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcoord_state\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdim\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     41\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/habitat/lib/python3.6/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    491\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    492\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 493\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    494\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    495\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/habitat/lib/python3.6/site-packages/torch/nn/modules/container.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m     90\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     91\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mmodule\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_modules\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 92\u001b[0;31m             \u001b[0minput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodule\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     93\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     94\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/habitat/lib/python3.6/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    491\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    492\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 493\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    494\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    495\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/habitat/lib/python3.6/site-packages/torch/nn/modules/activation.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m     97\u001b[0m     \u001b[0;34m@\u001b[0m\u001b[0mweak_script_method\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     98\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 99\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrelu\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minplace\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minplace\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    100\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    101\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mextra_repr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/habitat/lib/python3.6/site-packages/torch/nn/functional.py\u001b[0m in \u001b[0;36mrelu\u001b[0;34m(input, inplace)\u001b[0m\n\u001b[1;32m    941\u001b[0m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrelu_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    942\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 943\u001b[0;31m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrelu\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    944\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    945\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: CUDA out of memory. Tried to allocate 154.00 MiB (GPU 0; 31.72 GiB total capacity; 4.13 GiB already allocated; 62.38 MiB free; 29.23 MiB cached)"
     ]
    }
   ],
   "source": [
    "############## Hyperparameters ##############\n",
    "env_name = 'localnav'\n",
    "rl_env = LocalNav(agent)\n",
    "state_dim = 256\n",
    "action_dim = 3\n",
    "render = False\n",
    "solved_reward = -30         # stop training if avg_reward > solved_reward\n",
    "log_interval = 20           # print avg reward in the interval\n",
    "max_episodes = 50000        # max training episodes\n",
    "max_timesteps = 300         # max timesteps in one episode\n",
    "n_latent_var = 64           # number of variables in hidden layer\n",
    "update_timestep = 128       # update policy every n timesteps\n",
    "lr = 0.002\n",
    "betas = (0.9, 0.999)\n",
    "gamma = 0.99                # discount factor\n",
    "K_epochs = 4                # update policy for K epochs\n",
    "eps_clip = 0.2              # clip parameter for PPO\n",
    "random_seed = None\n",
    "#############################################\n",
    "\n",
    "if random_seed:\n",
    "    torch.manual_seed(random_seed)\n",
    "    env.seed(random_seed)\n",
    "\n",
    "memory = Memory()\n",
    "ppo = PPO(state_dim, action_dim, n_latent_var, lr, betas, gamma, K_epochs, eps_clip)\n",
    "print(lr,betas)\n",
    "\n",
    "# logging variables\n",
    "running_reward = 0\n",
    "avg_length = 0\n",
    "timestep = 0\n",
    "\n",
    "# training loop\n",
    "for i_episode in range(1, max_episodes+1):\n",
    "    state = rl_env.reset()\n",
    "    for t in range(max_timesteps):\n",
    "        timestep += 1\n",
    "\n",
    "        # Running policy_old:\n",
    "        action = ppo.policy_old.act(state, memory)\n",
    "        state, reward, done, _ = rl_env.step(action)\n",
    "\n",
    "        # Saving reward and is_terminal:\n",
    "        memory.rewards.append(reward)\n",
    "        memory.is_terminals.append(done)\n",
    "\n",
    "        # update if its time\n",
    "        if timestep % update_timestep == 0:\n",
    "            ppo.update(memory)\n",
    "            memory.clear_memory()\n",
    "            timestep = 0\n",
    "\n",
    "        running_reward += reward\n",
    "        if render:\n",
    "            rl_env.render()\n",
    "        if done:\n",
    "            break\n",
    "\n",
    "    avg_length += t\n",
    "    \n",
    "    # logging\n",
    "    if i_episode % log_interval == 0:\n",
    "        # stop training if avg_reward > solved_reward\n",
    "        if running_reward > (log_interval*solved_reward):\n",
    "            print(\"########## Solved! ##########\")\n",
    "            torch.save(ppo.policy.state_dict(), './PPO_{}.pth'.format(env_name))\n",
    "            break\n",
    "        avg_length = int(avg_length/log_interval)\n",
    "        running_reward = int((running_reward/log_interval))\n",
    "\n",
    "        print('Episode {} \\t avg length: {} \\t reward: {}'.format(i_episode, avg_length, running_reward))\n",
    "        running_reward = 0\n",
    "        avg_length = 0"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "habitat",
   "language": "python",
   "name": "habitat"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
