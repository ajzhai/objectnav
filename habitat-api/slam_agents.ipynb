{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Bad key \"text.kerning_factor\" on line 4 in\n",
      "/home/ubuntu/anaconda3/envs/pytorch_p36/lib/python3.7/site-packages/matplotlib/mpl-data/stylelib/_classic_test_patch.mplstyle.\n",
      "You probably need to get an updated matplotlibrc file from\n",
      "https://github.com/matplotlib/matplotlib/blob/v3.1.3/matplotlibrc.template\n",
      "or from the matplotlib source distribution\n",
      "/home/ubuntu/anaconda3/envs/pytorch_p36/lib/python3.7/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:541: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n",
      "/home/ubuntu/anaconda3/envs/pytorch_p36/lib/python3.7/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:542: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n",
      "/home/ubuntu/anaconda3/envs/pytorch_p36/lib/python3.7/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:543: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n",
      "/home/ubuntu/anaconda3/envs/pytorch_p36/lib/python3.7/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:544: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n",
      "/home/ubuntu/anaconda3/envs/pytorch_p36/lib/python3.7/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:545: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n",
      "/home/ubuntu/anaconda3/envs/pytorch_p36/lib/python3.7/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:550: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n"
     ]
    }
   ],
   "source": [
    "#!/usr/bin/env python3\n",
    "\n",
    "# Copyright (c) Facebook, Inc. and its affiliates.\n",
    "# This source code is licensed under the MIT license found in the\n",
    "# LICENSE file in the root directory of this source tree.\n",
    "\n",
    "import argparse\n",
    "import os\n",
    "import random\n",
    "import sys\n",
    "import time\n",
    "from math import pi\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import orbslam2\n",
    "import PIL\n",
    "import cv2\n",
    "import requests\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "\n",
    "import habitat\n",
    "from habitat.config.default import get_config\n",
    "from habitat.sims.habitat_simulator.actions import HabitatSimActions\n",
    "from habitat_baselines.config.default import get_config as cfg_baseline\n",
    "from habitat_baselines.slambased.mappers import DirectDepthMapper\n",
    "from habitat_baselines.slambased.monodepth import MonoDepthEstimator\n",
    "from habitat_baselines.slambased.path_planners import DifferentiableStarPlanner\n",
    "from habitat_baselines.slambased.reprojection import (\n",
    "    get_direction,\n",
    "    get_distance,\n",
    "    habitat_goalpos_to_mapgoal_pos,\n",
    "    homogenize_p,\n",
    "    planned_path2tps,\n",
    "    project_tps_into_worldmap,\n",
    "    angle_to_pi_2_minus_pi_2 as norm_ang,\n",
    ")\n",
    "from habitat_baselines.slambased.utils import generate_2dgrid\n",
    "\n",
    "GOAL_SENSOR_UUID = \"pointgoal_with_gps_compass\"\n",
    "#GOAL_SENSOR_UUID = \"objectgoal\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def download(url, filename):\n",
    "    with open(filename, \"wb\") as f:\n",
    "        response = requests.get(url, stream=True)\n",
    "        total = response.headers.get(\"content-length\")\n",
    "        if total is None:\n",
    "            f.write(response.content)\n",
    "        else:\n",
    "            downloaded = 0\n",
    "            total = int(total)\n",
    "            for data in response.iter_content(\n",
    "                chunk_size=max(int(total / 1000), 1024 * 1024)\n",
    "            ):\n",
    "                downloaded += len(data)\n",
    "                f.write(data)\n",
    "                done = int(50 * downloaded / total)\n",
    "                sys.stdout.write(\n",
    "                    \"\\r[{}{}]\".format(\"â–ˆ\" * done, \".\" * (50 - done))\n",
    "                )\n",
    "                sys.stdout.flush()\n",
    "    sys.stdout.write(\"\\n\")\n",
    "\n",
    "\n",
    "def ResizePIL2(np_img, size=256):\n",
    "    im1 = PIL.Image.fromarray(np_img)\n",
    "    return np.array(im1.resize((size, size)))\n",
    "\n",
    "\n",
    "def make_good_config_for_orbslam2(config):\n",
    "    config.SIMULATOR.AGENT_0.SENSORS = [\"RGB_SENSOR\", \"DEPTH_SENSOR\"]\n",
    "    config.SIMULATOR.RGB_SENSOR.WIDTH = 640\n",
    "    config.SIMULATOR.RGB_SENSOR.HEIGHT = 480\n",
    "    config.SIMULATOR.DEPTH_SENSOR.WIDTH = 640\n",
    "    config.SIMULATOR.DEPTH_SENSOR.HEIGHT = 480\n",
    "    config.ORBSLAM2.CAMERA_HEIGHT = config.SIMULATOR.DEPTH_SENSOR.POSITION[\n",
    "        1\n",
    "    ]\n",
    "    config.ORBSLAM2.H_OBSTACLE_MIN = (\n",
    "        0.3 * config.ORBSLAM2.CAMERA_HEIGHT\n",
    "    )\n",
    "    config.ORBSLAM2.H_OBSTACLE_MAX = (\n",
    "        1.0 * config.ORBSLAM2.CAMERA_HEIGHT\n",
    "    )\n",
    "    config.ORBSLAM2.MIN_PTS_IN_OBSTACLE = (\n",
    "        config.SIMULATOR.DEPTH_SENSOR.WIDTH / 2.0\n",
    "    )\n",
    "    return\n",
    "\n",
    "\n",
    "from PIL import Image\n",
    "from examples.habitat_sim_py.utils.common import d3_40_colors_rgb\n",
    "import numpy as np\n",
    "\n",
    "def display_sample(rgb_obs, semantic_obs, depth_obs):\n",
    "    rgb_img = Image.fromarray(rgb_obs, mode=\"RGB\")\n",
    "    \n",
    "    semantic_img = Image.new(\"P\", (semantic_obs.shape[1], semantic_obs.shape[0]))\n",
    "    semantic_img.putpalette(d3_40_colors_rgb.flatten())\n",
    "    semantic_img.putdata((semantic_obs.flatten() % 40).astype(np.uint8))\n",
    "    semantic_img = semantic_img.convert(\"RGBA\")\n",
    "    \n",
    "    depth_img = Image.fromarray((depth_obs * 255).astype(np.uint8), mode=\"L\")\n",
    "\n",
    "    arr = [rgb_img, semantic_img, depth_img]\n",
    "    \n",
    "    titles = ['rgb', 'semantic', 'depth']\n",
    "    plt.figure(figsize=(12 ,8))\n",
    "    for i, data in enumerate(arr):\n",
    "        ax = plt.subplot(1, 3, i+1)\n",
    "        ax.axis('off')\n",
    "        ax.set_title(titles[i])\n",
    "        plt.imshow(data)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RandomAgent(object):\n",
    "    r\"\"\"Simplest agent, which returns random actions,\n",
    "    until reach the goal\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, config):\n",
    "        super(RandomAgent, self).__init__()\n",
    "        self.num_actions = config.NUM_ACTIONS\n",
    "        self.dist_threshold_to_stop = config.DIST_TO_STOP\n",
    "        self.reset()\n",
    "        return\n",
    "\n",
    "    def reset(self):\n",
    "        self.steps = 0\n",
    "        return\n",
    "\n",
    "    def update_internal_state(self, habitat_observation):\n",
    "        self.obs = habitat_observation\n",
    "        self.steps += 1\n",
    "        return\n",
    "\n",
    "    def is_goal_reached(self):\n",
    "        dist = self.obs[GOAL_SENSOR_UUID][0]\n",
    "        return dist <= self.dist_threshold_to_stop\n",
    "\n",
    "    def act(self, habitat_observation=None, random_prob=1.0):\n",
    "        self.update_internal_state(habitat_observation)\n",
    "        # Act\n",
    "        # Check if we are done\n",
    "        if self.is_goal_reached():\n",
    "            action = HabitatSimActions.STOP\n",
    "        else:\n",
    "            action = random.randint(0, self.num_actions - 1)\n",
    "        return {\"action\": action}\n",
    "\n",
    "\n",
    "class BlindAgent(RandomAgent):\n",
    "    def __init__(self, config):\n",
    "        super(BlindAgent, self).__init__()\n",
    "        self.pos_th = config.DIST_TO_STOP\n",
    "        self.angle_th = config.ANGLE_TH\n",
    "        self.reset()\n",
    "        return\n",
    "\n",
    "    def decide_what_to_do(self):\n",
    "        distance_to_goal = self.obs[GOAL_SENSOR_UUID][0]\n",
    "        angle_to_goal = norm_ang(np.array(self.obs[GOAL_SENSOR_UUID][1]))\n",
    "        command = HabitatSimActions.STOP\n",
    "        if distance_to_goal <= self.pos_th:\n",
    "            return command\n",
    "        if abs(angle_to_goal) < self.angle_th:\n",
    "            command = HabitatSimActions.MOVE_FORWARD\n",
    "        else:\n",
    "            if (angle_to_goal > 0) and (angle_to_goal < pi):\n",
    "                command = HabitatSimActions.TURN_LEFT\n",
    "            elif angle_to_goal > pi:\n",
    "                command = HabitatSimActions.TURN_RIGHT\n",
    "            elif (angle_to_goal < 0) and (angle_to_goal > -pi):\n",
    "                command = HabitatSimActions.TURN_RIGHT\n",
    "            else:\n",
    "                command = HabitatSimActions.TURN_LEFT\n",
    "\n",
    "        return command\n",
    "\n",
    "    def act(self, habitat_observation=None, random_prob=0.1):\n",
    "        self.update_internal_state(habitat_observation)\n",
    "        # Act\n",
    "        if self.is_goal_reached():\n",
    "            return HabitatSimActions.STOP\n",
    "        command = self.decide_what_to_do()\n",
    "        random_action = random.randint(0, self.num_actions - 1)\n",
    "        act_randomly = np.random.uniform(0, 1, 1) < random_prob\n",
    "        if act_randomly:\n",
    "            action = random_action\n",
    "        else:\n",
    "            action = command\n",
    "        return {\"action\": action}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ORBSLAM2Agent(RandomAgent):\n",
    "    def __init__(self, config, device=torch.device(\"cuda:0\"), print_attrs=False):\n",
    "        self.num_actions = config.NUM_ACTIONS\n",
    "        self.dist_threshold_to_stop = config.DIST_TO_STOP\n",
    "        self.slam_vocab_path = config.SLAM_VOCAB_PATH\n",
    "        assert os.path.isfile(self.slam_vocab_path)\n",
    "        self.slam_settings_path = config.SLAM_SETTINGS_PATH\n",
    "        assert os.path.isfile(self.slam_settings_path)\n",
    "        self.slam = orbslam2.System(\n",
    "            self.slam_vocab_path, self.slam_settings_path, orbslam2.Sensor.RGBD\n",
    "        )\n",
    "        self.slam.set_use_viewer(False)\n",
    "        self.slam.initialize()\n",
    "        self.device = device\n",
    "        self.map_size_meters = config.MAP_SIZE\n",
    "        self.map_cell_size = config.MAP_CELL_SIZE\n",
    "        self.pos_th = config.DIST_REACHED_TH\n",
    "        self.next_wp_th = config.NEXT_WAYPOINT_TH\n",
    "        self.angle_th = config.ANGLE_TH\n",
    "        self.obstacle_th = config.MIN_PTS_IN_OBSTACLE\n",
    "        self.depth_denorm = config.DEPTH_DENORM\n",
    "        self.planned_waypoints = []\n",
    "        self.mapper = DirectDepthMapper(\n",
    "            camera_height=config.CAMERA_HEIGHT,\n",
    "            near_th=config.D_OBSTACLE_MIN,\n",
    "            far_th=config.D_OBSTACLE_MAX,\n",
    "            h_min=config.H_OBSTACLE_MIN,\n",
    "            h_max=config.H_OBSTACLE_MAX,\n",
    "            map_size=config.MAP_SIZE,\n",
    "            map_cell_size=config.MAP_CELL_SIZE,\n",
    "            device=device,\n",
    "        )\n",
    "        self.planner = DifferentiableStarPlanner(\n",
    "            max_steps=config.PLANNER_MAX_STEPS,\n",
    "            preprocess=config.PREPROCESS_MAP,\n",
    "            beta=config.BETA,\n",
    "            device=device,\n",
    "        )\n",
    "        self.slam_to_world = 1.0\n",
    "        self.timestep = 0.1\n",
    "        self.timing = False\n",
    "        self.reset()\n",
    "        # print('self', self, dir(self))\n",
    "        if print_attrs:\n",
    "            for attr_name in dir(self):\n",
    "                if '__' not in attr_name:\n",
    "                    print(attr_name, getattr(self, attr_name), type(getattr(self, attr_name)))\n",
    "                    type_name = str(type(getattr(self, attr_name)))\n",
    "                    if 'numpy' in type_name or 'ensor' in type_name:\n",
    "                        print(getattr(getattr(self, attr_name), 'shape'))\n",
    "        return\n",
    "\n",
    "    def reset(self):\n",
    "        super(ORBSLAM2Agent, self).reset()\n",
    "        self.offset_to_goal = None\n",
    "        self.tracking_is_OK = False\n",
    "        self.waypointPose6D = None\n",
    "        self.unseen_obstacle = False\n",
    "        self.action_history = []\n",
    "        self.planned_waypoints = []\n",
    "        self.map2DObstacles = self.init_map2d()\n",
    "        n, ch, height, width = self.map2DObstacles.size()\n",
    "        self.coordinatesGrid = generate_2dgrid(height, width, False).to(\n",
    "            self.device\n",
    "        )\n",
    "        self.pose6D = self.init_pose6d()\n",
    "        self.action_history = []\n",
    "        self.pose6D_history = []\n",
    "        self.position_history = []\n",
    "        self.planned2Dpath = torch.zeros((0))\n",
    "        self.slam.reset()\n",
    "        self.cur_time = 0\n",
    "        self.to_do_list = []\n",
    "        self.waypoint_id = 0\n",
    "        self.needs_inspection = True\n",
    "        if self.device != torch.device(\"cpu\"):\n",
    "            torch.cuda.empty_cache()\n",
    "        return\n",
    "\n",
    "    def update_internal_state(self, habitat_observation):\n",
    "        super(ORBSLAM2Agent, self).update_internal_state(habitat_observation)\n",
    "        self.cur_time += self.timestep\n",
    "        rgb, depth = self.rgb_d_from_observation(habitat_observation)\n",
    "        t = time.time()\n",
    "        try:\n",
    "            self.slam.process_image_rgbd(rgb, depth, self.cur_time)\n",
    "            if self.timing:\n",
    "                print(time.time() - t, \"ORB_SLAM2\")\n",
    "            self.tracking_is_OK = str(self.slam.get_tracking_state()) == \"OK\"\n",
    "        except BaseException as e:\n",
    "            print(\"Warning!!!! ORBSLAM processing frame error\")\n",
    "            print(\"orbslam error:\", e)\n",
    "            self.tracking_is_OK = False\n",
    "        if not self.tracking_is_OK:\n",
    "            print(\"\\n\\n\\n\\nRESETTING MYSELF BC TRACKING NOT OKAY\\n\\n\\n\\n\")\n",
    "            print(\"SLAM TRACKING STATE:\", self.slam.get_tracking_state())\n",
    "            self.reset()\n",
    "        t = time.time()\n",
    "        self.set_offset_to_goal(habitat_observation)\n",
    "        if self.tracking_is_OK:\n",
    "            trajectory_history = np.array(self.slam.get_trajectory_points())\n",
    "            self.pose6D = homogenize_p(\n",
    "                torch.from_numpy(trajectory_history[-1])[1:]\n",
    "                .view(3, 4)\n",
    "                .to(self.device)\n",
    "            ).view(1, 4, 4)\n",
    "            self.trajectory_history = trajectory_history\n",
    "            if len(self.position_history) > 1:\n",
    "                previous_step = get_distance(\n",
    "                    self.pose6D.view(4, 4),\n",
    "                    torch.from_numpy(self.position_history[-1])\n",
    "                    .view(4, 4)\n",
    "                    .to(self.device),\n",
    "                )\n",
    "                if self.action_history[-1] == HabitatSimActions.MOVE_FORWARD:\n",
    "                    self.unseen_obstacle = (\n",
    "                        previous_step.item() <= 0.001\n",
    "                    )  # hardcoded threshold for not moving\n",
    "        current_obstacles = self.mapper(\n",
    "            torch.from_numpy(depth).to(self.device).squeeze(), self.pose6D\n",
    "        ).to(self.device)\n",
    "        self.current_obstacles = current_obstacles\n",
    "        self.map2DObstacles = torch.max(\n",
    "            self.map2DObstacles, current_obstacles.unsqueeze(0).unsqueeze(0)\n",
    "        )\n",
    "        if self.timing:\n",
    "            print(time.time() - t, \"Mapping\")\n",
    "        return True\n",
    "\n",
    "    def init_pose6d(self):\n",
    "        return torch.eye(4).float().to(self.device)\n",
    "\n",
    "    def map_size_in_cells(self):\n",
    "        return int(self.map_size_meters / self.map_cell_size)\n",
    "\n",
    "    def init_map2d(self):\n",
    "        return (\n",
    "            torch.zeros(\n",
    "                1, 1, self.map_size_in_cells(), self.map_size_in_cells()\n",
    "            )\n",
    "            .float()\n",
    "            .to(self.device)\n",
    "        )\n",
    "\n",
    "    def get_orientation_on_map(self):\n",
    "        self.pose6D = self.pose6D.view(1, 4, 4)\n",
    "        return torch.tensor(\n",
    "            [\n",
    "                [self.pose6D[0, 0, 0], self.pose6D[0, 0, 2]],\n",
    "                [self.pose6D[0, 2, 0], self.pose6D[0, 2, 2]],\n",
    "            ]\n",
    "        )\n",
    "\n",
    "    def get_position_on_map(self, do_floor=True):\n",
    "        return project_tps_into_worldmap(\n",
    "            self.pose6D.view(1, 4, 4),\n",
    "            self.map_cell_size,\n",
    "            self.map_size_meters,\n",
    "            do_floor,\n",
    "        )\n",
    "\n",
    "    def act(self, habitat_observation, random_prob=0.1):\n",
    "        # Update internal state\n",
    "        t = time.time()\n",
    "        cc = 0\n",
    "        update_is_ok = self.update_internal_state(habitat_observation)\n",
    "        while not update_is_ok:\n",
    "            update_is_ok = self.update_internal_state(habitat_observation)\n",
    "            cc += 1\n",
    "            if cc > 2:\n",
    "                break\n",
    "        if self.timing:\n",
    "            print(time.time() - t, \" s, update internal state\")\n",
    "        self.position_history.append(\n",
    "            self.pose6D.detach().cpu().numpy().reshape(1, 4, 4)\n",
    "        )\n",
    "        success = self.is_goal_reached()\n",
    "        if success:\n",
    "            print(\"\\n\\n\\nGOAL IS BELIEVED TO BE REACHED before planning\\n\\n\\n\")\n",
    "            action = HabitatSimActions.STOP\n",
    "            self.action_history.append(action)\n",
    "            return {\"action\": action}\n",
    "        # Plan action\n",
    "        t = time.time()\n",
    "        if self.needs_inspection:\n",
    "            # do a left turn 360\n",
    "            # each turn is 30 degrees\n",
    "            self.to_do_list.extend(\n",
    "                [2] * 36\n",
    "            )\n",
    "            self.needs_inspection = False\n",
    "\n",
    "        if len(self.to_do_list) > 0:\n",
    "            # pop from to do list queue\n",
    "            action = self.to_do_list.pop(0)\n",
    "            self.action_history.append(action)\n",
    "            return {\"action\": action}\n",
    "            \n",
    "        self.planned2Dpath, self.planned_waypoints = self.plan_path()\n",
    "        if self.timing:\n",
    "            print(time.time() - t, \" s, Planning\")\n",
    "        t = time.time()\n",
    "        # Act\n",
    "        if self.waypointPose6D is None:\n",
    "            self.waypointPose6D = self.get_valid_waypoint_pose6d()\n",
    "        if (\n",
    "            self.is_waypoint_reached(self.waypointPose6D)\n",
    "            or not self.tracking_is_OK\n",
    "        ):\n",
    "            self.waypointPose6D = self.get_valid_waypoint_pose6d()\n",
    "            self.waypoint_id += 1\n",
    "        action = self.decide_what_to_do()\n",
    "        # May be random? ALEX: LETS NOT\n",
    "#         random_action = random.randint(0, self.num_actions - 1)\n",
    "#         what_to_do = np.random.uniform(0, 1, 1)\n",
    "#         if what_to_do < random_prob:\n",
    "#             action = random_action\n",
    "        if self.timing:\n",
    "            print(time.time() - t, \" s, get action\")\n",
    "        self.action_history.append(action)\n",
    "        if action == 0:\n",
    "            print(\"\\n\\n\\nGOAL IS BELIEVED TO BE REACHED after planning\\n\\n\\n\")\n",
    "        return {\"action\": action}\n",
    "\n",
    "    def is_waypoint_good(self, pose6d):\n",
    "        p_init = self.pose6D.squeeze()\n",
    "        dist_diff = get_distance(p_init, pose6d)\n",
    "        valid = dist_diff > self.next_wp_th\n",
    "        return valid.item()\n",
    "\n",
    "    def is_waypoint_reached(self, pose6d):\n",
    "        p_init = self.pose6D.squeeze()\n",
    "        dist_diff = get_distance(p_init, pose6d)\n",
    "        reached = dist_diff <= self.pos_th\n",
    "        return reached.item()\n",
    "\n",
    "    def get_waypoint_dist_dir(self):\n",
    "        angle = get_direction(\n",
    "            self.pose6D.squeeze(), self.waypointPose6D.squeeze(), 0, 0\n",
    "        )\n",
    "        dist = get_distance(\n",
    "            self.pose6D.squeeze(), self.waypointPose6D.squeeze()\n",
    "        )\n",
    "        return torch.cat(\n",
    "            [\n",
    "                dist.view(1, 1),\n",
    "                torch.sin(angle).view(1, 1),\n",
    "                torch.cos(angle).view(1, 1),\n",
    "            ],\n",
    "            dim=1,\n",
    "        )\n",
    "\n",
    "    def get_valid_waypoint_pose6d(self):\n",
    "        p_next = self.planned_waypoints[0]\n",
    "        while not self.is_waypoint_good(p_next):\n",
    "            if len(self.planned_waypoints) > 1:\n",
    "                self.planned_waypoints = self.planned_waypoints[1:]\n",
    "                p_next = self.planned_waypoints[0]\n",
    "            else:\n",
    "                p_next = self.estimatedGoalPos6D.squeeze()\n",
    "                break\n",
    "        return p_next\n",
    "\n",
    "    def is_goal_reached(self):\n",
    "        dist = self.obs[\"pointgoal_with_gps_compass\"][0]\n",
    "        return dist <= self.dist_threshold_to_stop\n",
    "    \n",
    "    def set_offset_to_goal(self, observation):\n",
    "        \"\"\" ID mappings\"\"\"\n",
    "        obj_to_id = {'chair': 0, 'table': 1, 'picture': 2, 'cabinet': 3, 'cushion': 4, 'sofa': 5, 'bed': 6, 'chest_of_drawers': 7, 'plant': 8, 'sink': 9, 'toilet': 10, 'stool': 11, 'towel': 12, 'tv_monitor': 13, 'shower': 14, 'bathtub': 15, 'counter': 16, 'fireplace': 17, 'gym_equipment': 18, 'seating': 19, 'clothes': 20}\n",
    "        id_to_obj = {obj_to_id[key]: key for key in obj_to_id}\n",
    "        # [distance to goal in metres, angle to goal in radians]\n",
    "        if GOAL_SENSOR_UUID == 'objectgoal':\n",
    "            self.obs[\"pointgoal_with_gps_compass\"] = np.array([1.0,3])\n",
    "            print('class observation from goal', id_to_obj[observation[GOAL_SENSOR_UUID][0]])\n",
    "\n",
    "        self.offset_to_goal = (\n",
    "                torch.from_numpy(observation[\"pointgoal_with_gps_compass\"])\n",
    "                .float()\n",
    "                .to(self.device)\n",
    "        )\n",
    "        print('ostensible observation from goal', self.offset_to_goal)\n",
    "        self.estimatedGoalPos2D = habitat_goalpos_to_mapgoal_pos(\n",
    "            self.offset_to_goal,\n",
    "            self.pose6D.squeeze(),\n",
    "            self.map_cell_size,\n",
    "            self.map_size_meters,\n",
    "        )\n",
    "        self.estimatedGoalPos6D = planned_path2tps(\n",
    "            [self.estimatedGoalPos2D],\n",
    "            self.map_cell_size,\n",
    "            self.map_size_meters,\n",
    "            1.0,\n",
    "        ).to(self.device)[0]\n",
    "        # {'STOP': 0, 'MOVE_FORWARD': 1, 'TURN_LEFT': 2, 'TURN_RIGHT': 3, 'LOOK_UP': 4, 'LOOK_DOWN': 5}\n",
    "        # https://github.com/facebookresearch/habitat-api/blob/master/habitat/sims/habitat_simulator/actions.py\n",
    "        log_attrs = [\n",
    "            'pose6D', 'estimatedGoalPos6D', 'action_history', 'cur_time', 'steps'\n",
    "        ]\n",
    "        for attr in log_attrs:\n",
    "            print(attr, getattr(self, attr))\n",
    "        if len(self.position_history) > 0:\n",
    "            print(\"most recent position:\", self.position_history[-1])\n",
    "        print('num obstacles:', self.map2DObstacles.sum())\n",
    "        if self.map2DObstacles.sum() > 0:\n",
    "            map_dir = 'maps/' + GOAL_SENSOR_UUID\n",
    "            if not os.path.exists(map_dir):\n",
    "                os.mkdir(map_dir)\n",
    "            suffix = str(int(time.time())) + '_' + str(int(self.map2DObstacles.sum()))\n",
    "            np.savetxt(map_dir + '/obstacle_map_' +\n",
    "                       suffix + '.csv',\n",
    "                       self.map2DObstacles[0,0].cpu().numpy())\n",
    "            PIL.Image.fromarray(observation['rgb']).save(map_dir + '/rgb_' + suffix + '.png')\n",
    "            PIL.Image.fromarray(\n",
    "                (observation['depth'] * 255).astype(np.uint8).squeeze(2), 'L'\n",
    "            ).save(map_dir + '/depth_' + suffix + '.png')\n",
    "        return\n",
    "\n",
    "    def rgb_d_from_observation(self, habitat_observation):\n",
    "        rgb = habitat_observation[\"rgb\"]\n",
    "        depth = None\n",
    "        if \"depth\" in habitat_observation:\n",
    "            depth = self.depth_denorm * habitat_observation[\"depth\"]\n",
    "        return rgb, depth\n",
    "\n",
    "    def prev_plan_is_not_valid(self):\n",
    "        if len(self.planned2Dpath) == 0:\n",
    "            return True\n",
    "        pp = torch.cat(self.planned2Dpath).detach().cpu().view(-1, 2)\n",
    "        binary_map = self.map2DObstacles.squeeze().detach() >= self.obstacle_th\n",
    "        obstacles_on_path = (\n",
    "            binary_map[pp[:, 0].long(), pp[:, 1].long()]\n",
    "        ).long().sum().item() > 0\n",
    "        return obstacles_on_path  # obstacles_nearby or  obstacles_on_path\n",
    "\n",
    "    def rawmap2_planner_ready(self, rawmap, start_map, goal_map):\n",
    "        map1 = (rawmap / float(self.obstacle_th)) ** 2\n",
    "        map1 = (\n",
    "            torch.clamp(map1, min=0, max=1.0)\n",
    "            - start_map\n",
    "            - F.max_pool2d(goal_map, 3, stride=1, padding=1)\n",
    "        )\n",
    "        return torch.relu(map1)\n",
    "\n",
    "    def plan_path(self, overwrite=False):\n",
    "        t = time.time()\n",
    "        if (\n",
    "            (not self.prev_plan_is_not_valid())\n",
    "            and (not overwrite)\n",
    "            and (len(self.planned_waypoints) > 0)\n",
    "        ):\n",
    "            return self.planned2Dpath, self.planned_waypoints\n",
    "        self.waypointPose6D = None\n",
    "        current_pos = self.get_position_on_map()\n",
    "        start_map = torch.zeros_like(self.map2DObstacles).to(self.device)\n",
    "        start_map[\n",
    "            0, 0, current_pos[0, 0].long(), current_pos[0, 1].long()\n",
    "        ] = 1.0\n",
    "        goal_map = torch.zeros_like(self.map2DObstacles).to(self.device)\n",
    "        goal_map[\n",
    "            0,\n",
    "            0,\n",
    "            self.estimatedGoalPos2D[0, 0].long(),\n",
    "            self.estimatedGoalPos2D[0, 1].long(),\n",
    "        ] = 1.0\n",
    "        path, cost = self.planner(\n",
    "            self.rawmap2_planner_ready(\n",
    "                self.map2DObstacles, start_map, goal_map\n",
    "            ).to(self.device),\n",
    "            self.coordinatesGrid.to(self.device),\n",
    "            goal_map.to(self.device),\n",
    "            start_map.to(self.device),\n",
    "        )\n",
    "        if len(path) == 0:\n",
    "            return path, []\n",
    "        if self.timing:\n",
    "            print(time.time() - t, \" s, Planning\")\n",
    "        t = time.time()\n",
    "        planned_waypoints = planned_path2tps(\n",
    "            path, self.map_cell_size, self.map_size_meters, 1.0, False\n",
    "        ).to(self.device)\n",
    "        return path, planned_waypoints\n",
    "\n",
    "    def planner_prediction_to_command(self, p_next):\n",
    "        command = HabitatSimActions.STOP\n",
    "        p_init = self.pose6D.squeeze()\n",
    "        d_angle_rot_th = self.angle_th\n",
    "        pos_th = self.pos_th\n",
    "        if get_distance(p_init, p_next) <= pos_th:\n",
    "            return command\n",
    "        d_angle = norm_ang(\n",
    "            get_direction(p_init, p_next, ang_th=d_angle_rot_th, pos_th=pos_th)\n",
    "        )\n",
    "        if abs(d_angle) < d_angle_rot_th:\n",
    "            command = HabitatSimActions.MOVE_FORWARD\n",
    "        else:\n",
    "            if (d_angle > 0) and (d_angle < pi):\n",
    "                command = HabitatSimActions.TURN_LEFT\n",
    "            elif d_angle > pi:\n",
    "                command = HabitatSimActions.TURN_RIGHT\n",
    "            elif (d_angle < 0) and (d_angle > -pi):\n",
    "                command = HabitatSimActions.TURN_RIGHT\n",
    "            else:\n",
    "                command = HabitatSimActions.TURN_LEFT\n",
    "        return command\n",
    "\n",
    "    def decide_what_to_do(self):\n",
    "        action = None\n",
    "        if self.is_goal_reached():\n",
    "            action = HabitatSimActions.STOP\n",
    "            return {\"action\": action}\n",
    "        if self.unseen_obstacle:\n",
    "            command = HabitatSimActions.TURN_RIGHT\n",
    "            return command\n",
    "        command = HabitatSimActions.STOP\n",
    "        command = self.planner_prediction_to_command(self.waypointPose6D)\n",
    "        return command"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ORBSLAM2MonodepthAgent(ORBSLAM2Agent):\n",
    "    def __init__(\n",
    "        self,\n",
    "        config,\n",
    "        device=torch.device(\"cuda:0\"),\n",
    "        monocheckpoint=\"habitat_baselines/slambased/data/mp3d_resnet50.pth\",\n",
    "    ):\n",
    "        self.num_actions = config.NUM_ACTIONS\n",
    "        self.dist_threshold_to_stop = config.DIST_TO_STOP\n",
    "        self.slam_vocab_path = config.SLAM_VOCAB_PATH\n",
    "        assert os.path.isfile(self.slam_vocab_path)\n",
    "        self.slam_settings_path = config.SLAM_SETTINGS_PATH\n",
    "        assert os.path.isfile(self.slam_settings_path)\n",
    "        self.slam = orbslam2.System(\n",
    "            self.slam_vocab_path, self.slam_settings_path, orbslam2.Sensor.RGBD\n",
    "        )\n",
    "        self.slam.set_use_viewer(False)\n",
    "        self.slam.initialize()\n",
    "        self.device = device\n",
    "        self.map_size_meters = config.MAP_SIZE\n",
    "        self.map_cell_size = config.MAP_CELL_SIZE\n",
    "        self.pos_th = config.DIST_REACHED_TH\n",
    "        self.next_wp_th = config.NEXT_WAYPOINT_TH\n",
    "        self.angle_th = config.ANGLE_TH\n",
    "        self.obstacle_th = config.MIN_PTS_IN_OBSTACLE\n",
    "        self.depth_denorm = config.DEPTH_DENORM\n",
    "        self.planned_waypoints = []\n",
    "        self.mapper = DirectDepthMapper(\n",
    "            camera_height=config.CAMERA_HEIGHT,\n",
    "            near_th=config.D_OBSTACLE_MIN,\n",
    "            far_th=config.D_OBSTACLE_MAX,\n",
    "            h_min=config.H_OBSTACLE_MIN,\n",
    "            h_max=config.H_OBSTACLE_MAX,\n",
    "            map_size=config.MAP_SIZE,\n",
    "            map_cell_size=config.MAP_CELL_SIZE,\n",
    "            device=device,\n",
    "        )\n",
    "        self.planner = DifferentiableStarPlanner(\n",
    "            max_steps=config.PLANNER_MAX_STEPS,\n",
    "            preprocess=config.PREPROCESS_MAP,\n",
    "            beta=config.BETA,\n",
    "            device=device,\n",
    "        )\n",
    "        self.slam_to_world = 1.0\n",
    "        self.timestep = 0.1\n",
    "        self.timing = False\n",
    "        self.checkpoint = monocheckpoint\n",
    "        if not os.path.isfile(self.checkpoint):\n",
    "            mp3d_url = \"http://cmp.felk.cvut.cz/~mishkdmy/navigation/mp3d_ft_monodepth_resnet50.pth\"\n",
    "            suncg_me_url = \"http://cmp.felk.cvut.cz/~mishkdmy/navigation/suncg_me_resnet.pth\"\n",
    "            suncg_mf_url = \"http://cmp.felk.cvut.cz/~mishkdmy/navigation/suncg_mf_resnet.pth\"\n",
    "            url = mp3d_url\n",
    "            print(\"No monodepth checkpoint found. Downloading...\", url)\n",
    "            download(url, self.checkpoint)\n",
    "        self.monodepth = MonoDepthEstimator(self.checkpoint)\n",
    "        self.reset()\n",
    "        return\n",
    "\n",
    "    def rgb_d_from_observation(self, habitat_observation):\n",
    "        rgb = habitat_observation[\"rgb\"]\n",
    "        depth = ResizePIL2(\n",
    "            self.monodepth.compute_depth(\n",
    "                PIL.Image.fromarray(rgb).resize((320, 320))\n",
    "            ),\n",
    "            256,\n",
    "        )  # /1.75\n",
    "        depth[depth > 3.0] = 0\n",
    "        depth[depth < 0.1] = 0\n",
    "        return rgb, np.array(depth).astype(np.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "parser = argparse.ArgumentParser()\n",
    "parser.add_argument(\n",
    "    \"--agent-type\",\n",
    "    default=\"orbslam2-rgbd\",\n",
    "    choices=[\"blind\", \"orbslam2-rgbd\", \"orbslam2-rgb-monod\"],\n",
    ")\n",
    "parser.add_argument(\n",
    "    \"--task-config\", type=str, default=\"tasks/pointnav_rgbd.yaml\"\n",
    ")\n",
    "parser.add_argument(\n",
    "    \"--goal-sensor-uuid\", type=str, default=\"pointgoal_with_gps_compass\"\n",
    ")\n",
    "args = parser.parse_args([\n",
    "    \"--task-config\", \"configs/tasks/objectnav_mp3d_fast.yaml\",\n",
    "    \"--goal-sensor-uuid\", \"objectgoal\"\n",
    "])\n",
    "\n",
    "global GOAL_SENSOR_UUID\n",
    "GOAL_SENSOR_UUID = args.goal_sensor_uuid\n",
    "\n",
    "config = get_config()\n",
    "agent_config = cfg_baseline()\n",
    "agent_config.defrost()\n",
    "config.defrost()\n",
    "config.ORBSLAM2 = agent_config.ORBSLAM2\n",
    "#config.BASELINE = agent_config.BASELINE\n",
    "make_good_config_for_orbslam2(config)\n",
    "\n",
    "if args.agent_type == \"blind\":\n",
    "    agent = BlindAgent(config.ORBSLAM2)\n",
    "elif args.agent_type == \"orbslam2-rgbd\":\n",
    "    agent = ORBSLAM2Agent(config.ORBSLAM2)\n",
    "elif args.agent_type == \"orbslam2-rgb-monod\":\n",
    "    agent = ORBSLAM2MonodepthAgent(config.ORBSLAM2)\n",
    "else:\n",
    "    raise ValueError(args.agent_type, \"is unknown type of agent\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2020-06-01 18:09:40,248 Initializing dataset ObjectNav-v1\n",
      "2020-06-01 18:09:40,283 initializing sim Sim-v0\n",
      "I0601 18:09:46.922451 3400 simulator.py:142] Loaded navmesh data/scene_datasets/mp3d/x8F5xyUWy9e/x8F5xyUWy9e.navmesh\n",
      "2020-06-01 18:09:46,930 Initializing task ObjectNav-v1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "original number of episodes: 30\n"
     ]
    }
   ],
   "source": [
    "task_config = habitat.get_config(config_paths=args.task_config)\n",
    "env = None\n",
    "env = habitat.Env(config=task_config)\n",
    "print(\"original number of episodes:\", len(env.episodes))\n",
    "env.episodes = [env.episodes[0]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(480, 640, 3) (480, 640, 1) [1] [-4.701501e-08] [-0.  0.]\n"
     ]
    }
   ],
   "source": [
    "observations = env.reset()\n",
    "print(\n",
    "    observations['rgb'].shape, observations['depth'].shape, observations['objectgoal'], \\\n",
    "    observations['compass'], observations['gps']\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture cap_out\n",
    "print(agent.action_history)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "class observation from goal chair\n",
      "ostensible observation from goal tensor([1., 3.], device='cuda:0')\n",
      "pose6D tensor([[1., 0., 0., 0.],\n",
      "        [0., 1., 0., 0.],\n",
      "        [0., 0., 1., 0.],\n",
      "        [0., 0., 0., 1.]], device='cuda:0')\n",
      "estimatedGoalPos6D tensor([[ 0.1000,  0.0000,  0.0000, -0.1000],\n",
      "        [ 0.0000,  0.1000,  0.0000,  0.1000],\n",
      "        [ 0.0000,  0.0000,  0.1000, -0.9000],\n",
      "        [ 0.0000,  0.0000,  0.0000,  1.0000]], device='cuda:0')\n",
      "action_history []\n",
      "cur_time 0.1\n",
      "steps 1\n",
      "num obstacles: tensor(0., device='cuda:0')\n",
      "class observation from goal chair\n",
      "ostensible observation from goal tensor([1., 3.], device='cuda:0')\n",
      "pose6D tensor([[[1., 0., 0., 0.],\n",
      "         [0., 1., 0., 0.],\n",
      "         [0., 0., 1., 0.],\n",
      "         [0., 0., 0., 1.]]], device='cuda:0')\n",
      "estimatedGoalPos6D tensor([[ 0.1000,  0.0000,  0.0000, -0.1000],\n",
      "        [ 0.0000,  0.1000,  0.0000,  0.1000],\n",
      "        [ 0.0000,  0.0000,  0.1000, -0.9000],\n",
      "        [ 0.0000,  0.0000,  0.0000,  1.0000]], device='cuda:0')\n",
      "action_history [2]\n",
      "cur_time 0.2\n",
      "steps 2\n",
      "most recent position: [[[1. 0. 0. 0.]\n",
      "  [0. 1. 0. 0.]\n",
      "  [0. 0. 1. 0.]\n",
      "  [0. 0. 0. 1.]]]\n",
      "num obstacles: tensor(153387., device='cuda:0')\n",
      "class observation from goal chair\n",
      "ostensible observation from goal tensor([1., 3.], device='cuda:0')\n",
      "pose6D tensor([[[ 9.8531e-01,  2.8847e-04, -1.7076e-01, -2.0493e-03],\n",
      "         [-3.3296e-04,  1.0000e+00, -2.3192e-04, -2.9885e-05],\n",
      "         [ 1.7076e-01,  2.8536e-04,  9.8531e-01, -6.1080e-03],\n",
      "         [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  1.0000e+00]]],\n",
      "       device='cuda:0')\n",
      "estimatedGoalPos6D tensor([[ 0.1000,  0.0000,  0.0000,  0.1000],\n",
      "        [ 0.0000,  0.1000,  0.0000,  0.1000],\n",
      "        [ 0.0000,  0.0000,  0.1000, -1.0000],\n",
      "        [ 0.0000,  0.0000,  0.0000,  1.0000]], device='cuda:0')\n",
      "action_history [2, 2]\n",
      "cur_time 0.30000000000000004\n",
      "steps 3\n",
      "most recent position: [[[ 9.8531300e-01  2.8846780e-04 -1.7075779e-01 -2.0493155e-03]\n",
      "  [-3.3295937e-04  9.9999994e-01 -2.3191572e-04 -2.9884752e-05]\n",
      "  [ 1.7075771e-01  2.8536498e-04  9.8531300e-01 -6.1080493e-03]\n",
      "  [ 0.0000000e+00  0.0000000e+00  0.0000000e+00  1.0000000e+00]]]\n",
      "num obstacles: tensor(243467., device='cuda:0')\n",
      "class observation from goal chair\n",
      "ostensible observation from goal tensor([1., 3.], device='cuda:0')\n",
      "pose6D tensor([[[ 9.3483e-01, -6.4907e-04, -3.5510e-01,  1.4338e-02],\n",
      "         [ 6.0801e-04,  1.0000e+00, -2.2721e-04,  4.0483e-04],\n",
      "         [ 3.5510e-01, -3.5081e-06,  9.3483e-01, -3.9462e-03],\n",
      "         [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  1.0000e+00]]],\n",
      "       device='cuda:0')\n",
      "estimatedGoalPos6D tensor([[ 0.1000,  0.0000,  0.0000,  0.3000],\n",
      "        [ 0.0000,  0.1000,  0.0000,  0.1000],\n",
      "        [ 0.0000,  0.0000,  0.1000, -0.9000],\n",
      "        [ 0.0000,  0.0000,  0.0000,  1.0000]], device='cuda:0')\n",
      "action_history [2, 2, 2]\n",
      "cur_time 0.4\n",
      "steps 4\n",
      "most recent position: [[[ 9.3482763e-01 -6.4906885e-04 -3.5510123e-01  1.4338206e-02]\n",
      "  [ 6.0801336e-04  9.9999976e-01 -2.2720578e-04  4.0483376e-04]\n",
      "  [ 3.5510132e-01 -3.5080654e-06  9.3482780e-01 -3.9462266e-03]\n",
      "  [ 0.0000000e+00  0.0000000e+00  0.0000000e+00  1.0000000e+00]]]\n",
      "num obstacles: tensor(327231., device='cuda:0')\n",
      "class observation from goal chair\n",
      "ostensible observation from goal tensor([1., 3.], device='cuda:0')\n",
      "pose6D tensor([[[ 8.5008e-01,  6.7632e-04, -5.2665e-01,  2.9670e-02],\n",
      "         [-5.4867e-04,  1.0000e+00,  3.9857e-04, -5.4774e-04],\n",
      "         [ 5.2665e-01, -4.9858e-05,  8.5008e-01,  3.6160e-03],\n",
      "         [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  1.0000e+00]]],\n",
      "       device='cuda:0')\n",
      "estimatedGoalPos6D tensor([[ 0.1000,  0.0000,  0.0000,  0.5000],\n",
      "        [ 0.0000,  0.1000,  0.0000,  0.1000],\n",
      "        [ 0.0000,  0.0000,  0.1000, -0.9000],\n",
      "        [ 0.0000,  0.0000,  0.0000,  1.0000]], device='cuda:0')\n",
      "action_history [2, 2, 2, 2]\n",
      "cur_time 0.5\n",
      "steps 5\n",
      "most recent position: [[[ 8.5008425e-01  6.7632349e-04 -5.2664620e-01  2.9669683e-02]\n",
      "  [-5.4867461e-04  9.9999976e-01  3.9856668e-04 -5.4774299e-04]\n",
      "  [ 5.2664632e-01 -4.9857845e-05  8.5008448e-01  3.6160247e-03]\n",
      "  [ 0.0000000e+00  0.0000000e+00  0.0000000e+00  1.0000000e+00]]]\n",
      "num obstacles: tensor(388429., device='cuda:0')\n",
      "class observation from goal chair\n",
      "ostensible observation from goal tensor([1., 3.], device='cuda:0')\n",
      "pose6D tensor([[[ 7.4258e-01,  6.4778e-04, -6.6976e-01,  3.4001e-02],\n",
      "         [ 2.0936e-04,  1.0000e+00,  1.1993e-03, -1.8307e-03],\n",
      "         [ 6.6976e-01, -1.0308e-03,  7.4258e-01,  6.7390e-03],\n",
      "         [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  1.0000e+00]]],\n",
      "       device='cuda:0')\n",
      "estimatedGoalPos6D tensor([[ 0.1000,  0.0000,  0.0000,  0.6000],\n",
      "        [ 0.0000,  0.1000,  0.0000,  0.1000],\n",
      "        [ 0.0000,  0.0000,  0.1000, -0.8000],\n",
      "        [ 0.0000,  0.0000,  0.0000,  1.0000]], device='cuda:0')\n",
      "action_history [2, 2, 2, 2, 2]\n",
      "cur_time 0.6\n",
      "steps 6\n",
      "most recent position: [[[ 7.4257809e-01  6.4778008e-04 -6.6975933e-01  3.4000896e-02]\n",
      "  [ 2.0935964e-04  9.9999928e-01  1.1993047e-03 -1.8306782e-03]\n",
      "  [ 6.6975945e-01 -1.0307977e-03  7.4257725e-01  6.7390492e-03]\n",
      "  [ 0.0000000e+00  0.0000000e+00  0.0000000e+00  1.0000000e+00]]]\n",
      "num obstacles: tensor(439615., device='cuda:0')\n",
      "class observation from goal chair\n",
      "ostensible observation from goal tensor([1., 3.], device='cuda:0')\n",
      "pose6D tensor([[[ 6.1297e-01,  1.5671e-03, -7.9011e-01,  3.5509e-02],\n",
      "         [-2.1790e-03,  1.0000e+00,  2.9297e-04,  2.9305e-04],\n",
      "         [ 7.9011e-01,  1.5420e-03,  6.1297e-01,  1.2364e-02],\n",
      "         [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  1.0000e+00]]],\n",
      "       device='cuda:0')\n",
      "estimatedGoalPos6D tensor([[ 0.1000,  0.0000,  0.0000,  0.8000],\n",
      "        [ 0.0000,  0.1000,  0.0000,  0.1000],\n",
      "        [ 0.0000,  0.0000,  0.1000, -0.7000],\n",
      "        [ 0.0000,  0.0000,  0.0000,  1.0000]], device='cuda:0')\n",
      "action_history [2, 2, 2, 2, 2, 2]\n",
      "cur_time 0.7\n",
      "steps 7\n",
      "most recent position: [[[ 6.1296648e-01  1.5671246e-03 -7.9010737e-01  3.5508886e-02]\n",
      "  [-2.1789786e-03  9.9999750e-01  2.9297266e-04  2.9304501e-04]\n",
      "  [ 7.9010594e-01  1.5420450e-03  6.1296839e-01  1.2363866e-02]\n",
      "  [ 0.0000000e+00  0.0000000e+00  0.0000000e+00  1.0000000e+00]]]\n",
      "num obstacles: tensor(493347., device='cuda:0')\n",
      "class observation from goal chair\n",
      "ostensible observation from goal tensor([1., 3.], device='cuda:0')\n",
      "pose6D tensor([[[ 4.6532e-01,  1.4450e-03, -8.8514e-01,  3.6569e-02],\n",
      "         [-1.9006e-03,  1.0000e+00,  6.3330e-04,  7.4757e-04],\n",
      "         [ 8.8514e-01,  1.3877e-03,  4.6532e-01,  1.6562e-02],\n",
      "         [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  1.0000e+00]]],\n",
      "       device='cuda:0')\n",
      "estimatedGoalPos6D tensor([[ 0.1000,  0.0000,  0.0000,  0.9000],\n",
      "        [ 0.0000,  0.1000,  0.0000,  0.1000],\n",
      "        [ 0.0000,  0.0000,  0.1000, -0.5000],\n",
      "        [ 0.0000,  0.0000,  0.0000,  1.0000]], device='cuda:0')\n",
      "action_history [2, 2, 2, 2, 2, 2, 2]\n",
      "cur_time 0.7999999999999999\n",
      "steps 8\n",
      "most recent position: [[[ 4.6531636e-01  1.4449675e-03 -8.8514334e-01  3.6569208e-02]\n",
      "  [-1.9006461e-03  9.9999791e-01  6.3330203e-04  7.4757304e-04]\n",
      "  [ 8.8514245e-01  1.3876582e-03  4.6531820e-01  1.6561877e-02]\n",
      "  [ 0.0000000e+00  0.0000000e+00  0.0000000e+00  1.0000000e+00]]]\n",
      "num obstacles: tensor(522445., device='cuda:0')\n",
      "class observation from goal chair\n",
      "ostensible observation from goal tensor([1., 3.], device='cuda:0')\n",
      "pose6D tensor([[[ 3.0428e-01,  1.7485e-03, -9.5258e-01,  3.5557e-02],\n",
      "         [-1.0887e-03,  1.0000e+00,  1.4878e-03, -1.1938e-03],\n",
      "         [ 9.5258e-01,  5.8435e-04,  3.0429e-01,  1.8661e-02],\n",
      "         [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  1.0000e+00]]],\n",
      "       device='cuda:0')\n",
      "estimatedGoalPos6D tensor([[ 0.1000,  0.0000,  0.0000,  1.0000],\n",
      "        [ 0.0000,  0.1000,  0.0000,  0.1000],\n",
      "        [ 0.0000,  0.0000,  0.1000, -0.4000],\n",
      "        [ 0.0000,  0.0000,  0.0000,  1.0000]], device='cuda:0')\n",
      "action_history [2, 2, 2, 2, 2, 2, 2, 2]\n",
      "cur_time 0.8999999999999999\n",
      "steps 9\n",
      "most recent position: [[[ 3.0428439e-01  1.7485397e-03 -9.5257962e-01  3.5557285e-02]\n",
      "  [-1.0886944e-03  9.9999821e-01  1.4878173e-03 -1.1937571e-03]\n",
      "  [ 9.5258051e-01  5.8434869e-04  3.0428582e-01  1.8660614e-02]\n",
      "  [ 0.0000000e+00  0.0000000e+00  0.0000000e+00  1.0000000e+00]]]\n",
      "num obstacles: tensor(537554., device='cuda:0')\n",
      "class observation from goal chair\n",
      "ostensible observation from goal tensor([1., 3.], device='cuda:0')\n",
      "pose6D tensor([[[ 1.3288e-01,  2.4154e-03, -9.9113e-01,  3.7762e-02],\n",
      "         [-1.2807e-03,  1.0000e+00,  2.2653e-03, -2.1403e-03],\n",
      "         [ 9.9113e-01,  9.6831e-04,  1.3288e-01,  2.3550e-02],\n",
      "         [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  1.0000e+00]]],\n",
      "       device='cuda:0')\n",
      "estimatedGoalPos6D tensor([[ 0.1000,  0.0000,  0.0000,  1.1000],\n",
      "        [ 0.0000,  0.1000,  0.0000,  0.1000],\n",
      "        [ 0.0000,  0.0000,  0.1000, -0.2000],\n",
      "        [ 0.0000,  0.0000,  0.0000,  1.0000]], device='cuda:0')\n",
      "action_history [2, 2, 2, 2, 2, 2, 2, 2, 2]\n",
      "cur_time 0.9999999999999999\n",
      "steps 10\n",
      "most recent position: [[[ 1.3287726e-01  2.4154107e-03 -9.9112958e-01  3.7762493e-02]\n",
      "  [-1.2806818e-03  9.9999660e-01  2.2653234e-03 -2.1402831e-03]\n",
      "  [ 9.9113160e-01  9.6831156e-04  1.3287999e-01  2.3549942e-02]\n",
      "  [ 0.0000000e+00  0.0000000e+00  0.0000000e+00  1.0000000e+00]]]\n",
      "num obstacles: tensor(554054., device='cuda:0')\n",
      "class observation from goal chair\n",
      "ostensible observation from goal tensor([1., 3.], device='cuda:0')\n",
      "pose6D tensor([[[-4.0685e-02,  2.5450e-03, -9.9917e-01,  3.8374e-02],\n",
      "         [-3.9320e-04,  1.0000e+00,  2.5631e-03, -3.7496e-03],\n",
      "         [ 9.9917e-01,  4.9716e-04, -4.0684e-02,  2.1567e-02],\n",
      "         [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  1.0000e+00]]],\n",
      "       device='cuda:0')\n",
      "estimatedGoalPos6D tensor([[0.1000, 0.0000, 0.0000, 1.1000],\n",
      "        [0.0000, 0.1000, 0.0000, 0.1000],\n",
      "        [0.0000, 0.0000, 0.1000, 0.0000],\n",
      "        [0.0000, 0.0000, 0.0000, 1.0000]], device='cuda:0')\n",
      "action_history [2, 2, 2, 2, 2, 2, 2, 2, 2, 2]\n",
      "cur_time 1.0999999999999999\n",
      "steps 11\n",
      "most recent position: [[[-4.0684879e-02  2.5449898e-03 -9.9916887e-01  3.8374096e-02]\n",
      "  [-3.9320439e-04  9.9999660e-01  2.5631087e-03 -3.7495797e-03]\n",
      "  [ 9.9917197e-01  4.9715734e-04 -4.0683687e-02  2.1566920e-02]\n",
      "  [ 0.0000000e+00  0.0000000e+00  0.0000000e+00  1.0000000e+00]]]\n",
      "num obstacles: tensor(573571., device='cuda:0')\n",
      "class observation from goal chair\n",
      "ostensible observation from goal tensor([1., 3.], device='cuda:0')\n",
      "pose6D tensor([[[-2.1385e-01,  1.9514e-03, -9.7686e-01,  3.9664e-02],\n",
      "         [-2.8529e-04,  1.0000e+00,  2.0600e-03, -2.4189e-03],\n",
      "         [ 9.7687e-01,  7.1923e-04, -2.1385e-01,  2.2758e-02],\n",
      "         [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  1.0000e+00]]],\n",
      "       device='cuda:0')\n",
      "estimatedGoalPos6D tensor([[0.1000, 0.0000, 0.0000, 1.1000],\n",
      "        [0.0000, 0.1000, 0.0000, 0.1000],\n",
      "        [0.0000, 0.0000, 0.1000, 0.1000],\n",
      "        [0.0000, 0.0000, 0.0000, 1.0000]], device='cuda:0')\n",
      "action_history [2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2]\n",
      "cur_time 1.2\n",
      "steps 12\n",
      "most recent position: [[[-2.1385092e-01  1.9513600e-03 -9.7686434e-01  3.9664455e-02]\n",
      "  [-2.8528878e-04  9.9999791e-01  2.0600248e-03 -2.4189488e-03]\n",
      "  [ 9.7686619e-01  7.1922666e-04 -2.1384981e-01  2.2758283e-02]\n",
      "  [ 0.0000000e+00  0.0000000e+00  0.0000000e+00  1.0000000e+00]]]\n",
      "num obstacles: tensor(602507., device='cuda:0')\n",
      "class observation from goal chair\n",
      "ostensible observation from goal tensor([1., 3.], device='cuda:0')\n",
      "pose6D tensor([[[-3.8123e-01,  2.0628e-03, -9.2448e-01,  4.7964e-02],\n",
      "         [ 4.7822e-04,  1.0000e+00,  2.0341e-03, -2.0207e-03],\n",
      "         [ 9.2448e-01,  3.3337e-04, -3.8123e-01,  2.9722e-02],\n",
      "         [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  1.0000e+00]]],\n",
      "       device='cuda:0')\n",
      "estimatedGoalPos6D tensor([[0.1000, 0.0000, 0.0000, 1.1000],\n",
      "        [0.0000, 0.1000, 0.0000, 0.1000],\n",
      "        [0.0000, 0.0000, 0.1000, 0.3000],\n",
      "        [0.0000, 0.0000, 0.0000, 1.0000]], device='cuda:0')\n",
      "action_history [2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2]\n",
      "cur_time 1.3\n",
      "steps 13\n",
      "most recent position: [[[-3.8122833e-01  2.0628411e-03 -9.2447859e-01  4.7963541e-02]\n",
      "  [ 4.7821898e-04  9.9999774e-01  2.0341477e-03 -2.0206673e-03]\n",
      "  [ 9.2448080e-01  3.3337148e-04 -3.8122854e-01  2.9721698e-02]\n",
      "  [ 0.0000000e+00  0.0000000e+00  0.0000000e+00  1.0000000e+00]]]\n",
      "num obstacles: tensor(625401., device='cuda:0')\n",
      "class observation from goal chair\n",
      "ostensible observation from goal tensor([1., 3.], device='cuda:0')\n",
      "pose6D tensor([[[-5.3587e-01,  3.0170e-03, -8.4430e-01,  5.0440e-02],\n",
      "         [ 3.7293e-04,  9.9999e-01,  3.3367e-03, -7.2127e-03],\n",
      "         [ 8.4430e-01,  1.4732e-03, -5.3587e-01,  3.0906e-02],\n",
      "         [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  1.0000e+00]]],\n",
      "       device='cuda:0')\n",
      "estimatedGoalPos6D tensor([[0.1000, 0.0000, 0.0000, 1.0000],\n",
      "        [0.0000, 0.1000, 0.0000, 0.1000],\n",
      "        [0.0000, 0.0000, 0.1000, 0.5000],\n",
      "        [0.0000, 0.0000, 0.0000, 1.0000]], device='cuda:0')\n",
      "action_history [2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2]\n",
      "cur_time 1.4000000000000001\n",
      "steps 14\n",
      "most recent position: [[[-5.3586709e-01  3.0170376e-03 -8.4429699e-01  5.0439857e-02]\n",
      "  [ 3.7293026e-04  9.9999428e-01  3.3367160e-03 -7.2127488e-03]\n",
      "  [ 8.4430230e-01  1.4731726e-03 -5.3586513e-01  3.0906323e-02]\n",
      "  [ 0.0000000e+00  0.0000000e+00  0.0000000e+00  1.0000000e+00]]]\n",
      "num obstacles: tensor(646890., device='cuda:0')\n",
      "class observation from goal chair\n",
      "ostensible observation from goal tensor([1., 3.], device='cuda:0')\n",
      "pose6D tensor([[[-6.7515e-01,  2.8213e-03, -7.3767e-01,  4.5847e-02],\n",
      "         [ 4.6927e-04,  9.9999e-01,  3.3951e-03, -5.4226e-03],\n",
      "         [ 7.3768e-01,  1.9460e-03, -6.7515e-01,  3.2258e-02],\n",
      "         [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  1.0000e+00]]],\n",
      "       device='cuda:0')\n",
      "estimatedGoalPos6D tensor([[0.1000, 0.0000, 0.0000, 0.9000],\n",
      "        [0.0000, 0.1000, 0.0000, 0.1000],\n",
      "        [0.0000, 0.0000, 0.1000, 0.6000],\n",
      "        [0.0000, 0.0000, 0.0000, 1.0000]], device='cuda:0')\n",
      "action_history [2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2]\n",
      "cur_time 1.5000000000000002\n",
      "steps 15\n",
      "most recent position: [[[-6.7515165e-01  2.8213253e-03 -7.3767352e-01  4.5847286e-02]\n",
      "  [ 4.6927395e-04  9.9999404e-01  3.3951034e-03 -5.4225735e-03]\n",
      "  [ 7.3767883e-01  1.9460386e-03 -6.7514890e-01  3.2258388e-02]\n",
      "  [ 0.0000000e+00  0.0000000e+00  0.0000000e+00  1.0000000e+00]]]\n",
      "num obstacles: tensor(666050., device='cuda:0')\n",
      "class observation from goal chair\n",
      "ostensible observation from goal tensor([1., 3.], device='cuda:0')\n",
      "pose6D tensor([[[-7.9277e-01,  2.4714e-03, -6.0952e-01,  4.8987e-02],\n",
      "         [ 7.2988e-04,  9.9999e-01,  3.1053e-03, -6.7188e-03],\n",
      "         [ 6.0953e-01,  2.0169e-03, -7.9276e-01,  3.3866e-02],\n",
      "         [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  1.0000e+00]]],\n",
      "       device='cuda:0')\n",
      "estimatedGoalPos6D tensor([[0.1000, 0.0000, 0.0000, 0.8000],\n",
      "        [0.0000, 0.1000, 0.0000, 0.1000],\n",
      "        [0.0000, 0.0000, 0.1000, 0.8000],\n",
      "        [0.0000, 0.0000, 0.0000, 1.0000]], device='cuda:0')\n",
      "action_history [2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2]\n",
      "cur_time 1.6000000000000003\n",
      "steps 16\n",
      "most recent position: [[[-7.9276544e-01  2.4714030e-03 -6.0952181e-01  4.8987422e-02]\n",
      "  [ 7.2988344e-04  9.9999487e-01  3.1053263e-03 -6.7188470e-03]\n",
      "  [ 6.0952646e-01  2.0169155e-03 -7.9276323e-01  3.3866301e-02]\n",
      "  [ 0.0000000e+00  0.0000000e+00  0.0000000e+00  1.0000000e+00]]]\n",
      "num obstacles: tensor(683360., device='cuda:0')\n",
      "class observation from goal chair\n",
      "ostensible observation from goal tensor([1., 3.], device='cuda:0')\n",
      "pose6D tensor([[[-0.8862,  0.0035, -0.4632,  0.0513],\n",
      "         [ 0.0021,  1.0000,  0.0035, -0.0093],\n",
      "         [ 0.4632,  0.0021, -0.8862,  0.0373],\n",
      "         [ 0.0000,  0.0000,  0.0000,  1.0000]]], device='cuda:0')\n",
      "estimatedGoalPos6D tensor([[0.1000, 0.0000, 0.0000, 0.7000],\n",
      "        [0.0000, 0.1000, 0.0000, 0.1000],\n",
      "        [0.0000, 0.0000, 0.1000, 0.9000],\n",
      "        [0.0000, 0.0000, 0.0000, 1.0000]], device='cuda:0')\n",
      "action_history [2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2]\n",
      "cur_time 1.7000000000000004\n",
      "steps 17\n",
      "most recent position: [[[-0.88624334  0.00351957 -0.46320653  0.05130596]\n",
      "  [ 0.00213172  0.99999154  0.00351965 -0.00926578]\n",
      "  [ 0.46321502  0.00213184 -0.8862433   0.03729244]\n",
      "  [ 0.          0.          0.          1.        ]]]\n",
      "num obstacles: tensor(702828., device='cuda:0')\n",
      "class observation from goal chair\n",
      "ostensible observation from goal tensor([1., 3.], device='cuda:0')\n",
      "pose6D tensor([[[-0.9533,  0.0070, -0.3019,  0.0528],\n",
      "         [ 0.0061,  1.0000,  0.0041, -0.0146],\n",
      "         [ 0.3019,  0.0021, -0.9533,  0.0442],\n",
      "         [ 0.0000,  0.0000,  0.0000,  1.0000]]], device='cuda:0')\n",
      "estimatedGoalPos6D tensor([[0.1000, 0.0000, 0.0000, 0.5000],\n",
      "        [0.0000, 0.1000, 0.0000, 0.1000],\n",
      "        [0.0000, 0.0000, 0.1000, 1.0000],\n",
      "        [0.0000, 0.0000, 0.0000, 1.0000]], device='cuda:0')\n",
      "action_history [2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2]\n",
      "cur_time 1.8000000000000005\n",
      "steps 18\n",
      "most recent position: [[[-0.9533171   0.00704223 -0.3018889   0.05278901]\n",
      "  [ 0.00608368  0.99997294  0.00411532 -0.01462558]\n",
      "  [ 0.30190974  0.00208661 -0.9533342   0.04422764]\n",
      "  [ 0.          0.          0.          1.        ]]]\n",
      "num obstacles: tensor(723793., device='cuda:0')\n",
      "class observation from goal chair\n",
      "ostensible observation from goal tensor([1., 3.], device='cuda:0')\n",
      "pose6D tensor([[[-0.9913,  0.0084, -0.1315,  0.0539],\n",
      "         [ 0.0079,  1.0000,  0.0046, -0.0206],\n",
      "         [ 0.1315,  0.0036, -0.9913,  0.0527],\n",
      "         [ 0.0000,  0.0000,  0.0000,  1.0000]]], device='cuda:0')\n",
      "estimatedGoalPos6D tensor([[0.1000, 0.0000, 0.0000, 0.4000],\n",
      "        [0.0000, 0.1000, 0.0000, 0.1000],\n",
      "        [0.0000, 0.0000, 0.1000, 1.1000],\n",
      "        [0.0000, 0.0000, 0.0000, 1.0000]], device='cuda:0')\n",
      "action_history [2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2]\n",
      "cur_time 1.9000000000000006\n",
      "steps 19\n",
      "most recent position: [[[-0.9912816   0.00843215 -0.13149026  0.05388431]\n",
      "  [ 0.00789079  0.99995804  0.00463763 -0.02060328]\n",
      "  [ 0.13152385  0.00355964 -0.99130654  0.05272044]\n",
      "  [ 0.          0.          0.          1.        ]]]\n",
      "num obstacles: tensor(739131., device='cuda:0')\n",
      "class observation from goal chair\n",
      "ostensible observation from goal tensor([1., 3.], device='cuda:0')\n",
      "pose6D tensor([[[-0.9991,  0.0061,  0.0431,  0.0532],\n",
      "         [ 0.0064,  1.0000,  0.0055, -0.0263],\n",
      "         [-0.0431,  0.0058, -0.9991,  0.0617],\n",
      "         [ 0.0000,  0.0000,  0.0000,  1.0000]]], device='cuda:0')\n",
      "estimatedGoalPos6D tensor([[0.1000, 0.0000, 0.0000, 0.2000],\n",
      "        [0.0000, 0.1000, 0.0000, 0.1000],\n",
      "        [0.0000, 0.0000, 0.1000, 1.1000],\n",
      "        [0.0000, 0.0000, 0.0000, 1.0000]], device='cuda:0')\n",
      "action_history [2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2]\n",
      "cur_time 2.0000000000000004\n",
      "steps 20\n",
      "most recent position: [[[-0.9990507   0.0061258   0.0431309   0.05324683]\n",
      "  [ 0.00636871  0.9999646   0.0054968  -0.02630986]\n",
      "  [-0.04309571  0.00576627 -0.9990543   0.06173089]\n",
      "  [ 0.          0.          0.          1.        ]]]\n",
      "num obstacles: tensor(744642., device='cuda:0')\n",
      "class observation from goal chair\n",
      "ostensible observation from goal tensor([1., 3.], device='cuda:0')\n",
      "pose6D tensor([[[-0.9765,  0.0058,  0.2156,  0.0521],\n",
      "         [ 0.0069,  1.0000,  0.0047, -0.0288],\n",
      "         [-0.2155,  0.0061, -0.9765,  0.0656],\n",
      "         [ 0.0000,  0.0000,  0.0000,  1.0000]]], device='cuda:0')\n",
      "estimatedGoalPos6D tensor([[0.1000, 0.0000, 0.0000, 0.0000],\n",
      "        [0.0000, 0.1000, 0.0000, 0.1000],\n",
      "        [0.0000, 0.0000, 0.1000, 1.1000],\n",
      "        [0.0000, 0.0000, 0.0000, 1.0000]], device='cuda:0')\n",
      "action_history [2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2]\n",
      "cur_time 2.1000000000000005\n",
      "steps 21\n",
      "most recent position: [[[-0.9764685   0.0057504   0.21558405  0.05212894]\n",
      "  [ 0.00692329  0.9999651   0.00468578 -0.02877782]\n",
      "  [-0.21554957  0.00606807 -0.97647405  0.0655523 ]\n",
      "  [ 0.          0.          0.          1.        ]]]\n",
      "num obstacles: tensor(750827., device='cuda:0')\n",
      "class observation from goal chair\n",
      "ostensible observation from goal tensor([1., 3.], device='cuda:0')\n",
      "pose6D tensor([[[-0.9244,  0.0050,  0.3813,  0.0507],\n",
      "         [ 0.0069,  1.0000,  0.0038, -0.0323],\n",
      "         [-0.3813,  0.0061, -0.9244,  0.0732],\n",
      "         [ 0.0000,  0.0000,  0.0000,  1.0000]]], device='cuda:0')\n",
      "estimatedGoalPos6D tensor([[ 0.1000,  0.0000,  0.0000, -0.1000],\n",
      "        [ 0.0000,  0.1000,  0.0000,  0.1000],\n",
      "        [ 0.0000,  0.0000,  0.1000,  1.1000],\n",
      "        [ 0.0000,  0.0000,  0.0000,  1.0000]], device='cuda:0')\n",
      "action_history [2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2]\n",
      "cur_time 2.2000000000000006\n",
      "steps 22\n",
      "most recent position: [[[-0.92443734  0.00495054  0.38130182  0.05068363]\n",
      "  [ 0.0069165   0.9999689   0.00378569 -0.03230419]\n",
      "  [-0.3812712   0.00613691 -0.9244429   0.0732366 ]\n",
      "  [ 0.          0.          0.          1.        ]]]\n",
      "num obstacles: tensor(760211., device='cuda:0')\n",
      "class observation from goal chair\n",
      "ostensible observation from goal tensor([1., 3.], device='cuda:0')\n",
      "pose6D tensor([[[-0.8436,  0.0059,  0.5369,  0.0486],\n",
      "         [ 0.0080,  1.0000,  0.0016, -0.0274],\n",
      "         [-0.5368,  0.0056, -0.8437,  0.0654],\n",
      "         [ 0.0000,  0.0000,  0.0000,  1.0000]]], device='cuda:0')\n",
      "estimatedGoalPos6D tensor([[ 0.1000,  0.0000,  0.0000, -0.3000],\n",
      "        [ 0.0000,  0.1000,  0.0000,  0.1000],\n",
      "        [ 0.0000,  0.0000,  0.1000,  1.0000],\n",
      "        [ 0.0000,  0.0000,  0.0000,  1.0000]], device='cuda:0')\n",
      "action_history [2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2]\n",
      "cur_time 2.3000000000000007\n",
      "steps 23\n",
      "most recent position: [[[-0.84364694  0.00586185  0.5368663   0.04855311]\n",
      "  [ 0.00795775  0.99996704  0.00158677 -0.02743074]\n",
      "  [-0.53683937  0.00561092 -0.8436659   0.06538691]\n",
      "  [ 0.          0.          0.          1.        ]]]\n",
      "num obstacles: tensor(774677., device='cuda:0')\n",
      "class observation from goal chair\n",
      "ostensible observation from goal tensor([1., 3.], device='cuda:0')\n",
      "pose6D tensor([[[-7.3733e-01,  4.2796e-03,  6.7552e-01,  5.1330e-02],\n",
      "         [ 6.1040e-03,  9.9998e-01,  3.2734e-04, -1.8097e-02],\n",
      "         [-6.7551e-01,  4.3647e-03, -7.3734e-01,  5.0391e-02],\n",
      "         [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  1.0000e+00]]],\n",
      "       device='cuda:0')\n",
      "estimatedGoalPos6D tensor([[ 0.1000,  0.0000,  0.0000, -0.5000],\n",
      "        [ 0.0000,  0.1000,  0.0000,  0.1000],\n",
      "        [ 0.0000,  0.0000,  0.1000,  0.9000],\n",
      "        [ 0.0000,  0.0000,  0.0000,  1.0000]], device='cuda:0')\n",
      "action_history [2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2]\n",
      "cur_time 2.400000000000001\n",
      "steps 24\n",
      "most recent position: [[[-7.3732692e-01  4.2795888e-03  6.7552251e-01  5.1330175e-02]\n",
      "  [ 6.1039887e-03  9.9998134e-01  3.2734015e-04 -1.8096562e-02]\n",
      "  [-6.7550850e-01  4.3647382e-03 -7.3733932e-01  5.0391480e-02]\n",
      "  [ 0.0000000e+00  0.0000000e+00  0.0000000e+00  1.0000000e+00]]]\n",
      "num obstacles: tensor(794055., device='cuda:0')\n",
      "class observation from goal chair\n",
      "ostensible observation from goal tensor([1., 3.], device='cuda:0')\n",
      "pose6D tensor([[[-6.0857e-01,  4.5237e-03,  7.9349e-01,  5.0132e-02],\n",
      "         [ 6.4119e-03,  9.9998e-01, -7.8334e-04, -1.8317e-02],\n",
      "         [-7.9348e-01,  4.6110e-03, -6.0858e-01,  5.1729e-02],\n",
      "         [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  1.0000e+00]]],\n",
      "       device='cuda:0')\n",
      "estimatedGoalPos6D tensor([[ 0.1000,  0.0000,  0.0000, -0.6000],\n",
      "        [ 0.0000,  0.1000,  0.0000,  0.1000],\n",
      "        [ 0.0000,  0.0000,  0.1000,  0.8000],\n",
      "        [ 0.0000,  0.0000,  0.0000,  1.0000]], device='cuda:0')\n",
      "action_history [2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2]\n",
      "cur_time 2.500000000000001\n",
      "steps 25\n",
      "most recent position: [[[-6.0856533e-01  4.5237001e-03  7.9349089e-01  5.0131831e-02]\n",
      "  [ 6.4118542e-03  9.9997926e-01 -7.8334019e-04 -1.8317321e-02]\n",
      "  [-7.9347783e-01  4.6110344e-03 -6.0858172e-01  5.1728744e-02]\n",
      "  [ 0.0000000e+00  0.0000000e+00  0.0000000e+00  1.0000000e+00]]]\n",
      "num obstacles: tensor(817949., device='cuda:0')\n",
      "class observation from goal chair\n",
      "ostensible observation from goal tensor([1., 3.], device='cuda:0')\n",
      "pose6D tensor([[[-0.4609,  0.0047,  0.8875,  0.0508],\n",
      "         [ 0.0068,  1.0000, -0.0018, -0.0187],\n",
      "         [-0.8874,  0.0052, -0.4609,  0.0462],\n",
      "         [ 0.0000,  0.0000,  0.0000,  1.0000]]], device='cuda:0')\n",
      "estimatedGoalPos6D tensor([[ 0.1000,  0.0000,  0.0000, -0.7000],\n",
      "        [ 0.0000,  0.1000,  0.0000,  0.1000],\n",
      "        [ 0.0000,  0.0000,  0.1000,  0.7000],\n",
      "        [ 0.0000,  0.0000,  0.0000,  1.0000]], device='cuda:0')\n",
      "action_history [2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2]\n",
      "cur_time 2.600000000000001\n",
      "steps 26\n",
      "most recent position: [[[-0.460873    0.00474236  0.88745344  0.05076116]\n",
      "  [ 0.00681233  0.99997514 -0.00180586 -0.01870634]\n",
      "  [-0.88743997  0.00521335 -0.46089393  0.04618385]\n",
      "  [ 0.          0.          0.          1.        ]]]\n",
      "num obstacles: tensor(849363., device='cuda:0')\n",
      "class observation from goal chair\n",
      "ostensible observation from goal tensor([1., 3.], device='cuda:0')\n",
      "pose6D tensor([[[-0.3001,  0.0037,  0.9539,  0.0478],\n",
      "         [ 0.0047,  1.0000, -0.0024, -0.0189],\n",
      "         [-0.9539,  0.0038, -0.3002,  0.0496],\n",
      "         [ 0.0000,  0.0000,  0.0000,  1.0000]]], device='cuda:0')\n",
      "estimatedGoalPos6D tensor([[ 0.1000,  0.0000,  0.0000, -0.8000],\n",
      "        [ 0.0000,  0.1000,  0.0000,  0.1000],\n",
      "        [ 0.0000,  0.0000,  0.1000,  0.5000],\n",
      "        [ 0.0000,  0.0000,  0.0000,  1.0000]], device='cuda:0')\n",
      "action_history [2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2]\n",
      "cur_time 2.700000000000001\n",
      "steps 27\n",
      "most recent position: [[[-0.3001418   0.00366575  0.95388746  0.0478175 ]\n",
      "  [ 0.00470149  0.9999862  -0.00236358 -0.01890529]\n",
      "  [-0.95388305  0.00377528 -0.30015498  0.04961996]\n",
      "  [ 0.          0.          0.          1.        ]]]\n",
      "num obstacles: tensor(888297., device='cuda:0')\n",
      "class observation from goal chair\n",
      "ostensible observation from goal tensor([1., 3.], device='cuda:0')\n",
      "pose6D tensor([[[-1.2867e-01,  1.1321e-03,  9.9169e-01,  4.6826e-02],\n",
      "         [ 6.1090e-03,  9.9998e-01, -3.4896e-04, -2.9223e-02],\n",
      "         [-9.9167e-01,  6.0133e-03, -1.2867e-01,  4.4823e-02],\n",
      "         [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  1.0000e+00]]],\n",
      "       device='cuda:0')\n",
      "estimatedGoalPos6D tensor([[ 0.1000,  0.0000,  0.0000, -0.9000],\n",
      "        [ 0.0000,  0.1000,  0.0000,  0.1000],\n",
      "        [ 0.0000,  0.0000,  0.1000,  0.4000],\n",
      "        [ 0.0000,  0.0000,  0.0000,  1.0000]], device='cuda:0')\n",
      "action_history [2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2]\n",
      "cur_time 2.800000000000001\n",
      "steps 28\n",
      "most recent position: [[[-1.2866515e-01  1.1320987e-03  9.9168748e-01  4.6825811e-02]\n",
      "  [ 6.1090118e-03  9.9998128e-01 -3.4896098e-04 -2.9223116e-02]\n",
      "  [-9.9166924e-01  6.0133329e-03 -1.2866966e-01  4.4822894e-02]\n",
      "  [ 0.0000000e+00  0.0000000e+00  0.0000000e+00  1.0000000e+00]]]\n",
      "num obstacles: tensor(928881., device='cuda:0')\n",
      "class observation from goal chair\n",
      "ostensible observation from goal tensor([1., 3.], device='cuda:0')\n",
      "pose6D tensor([[[ 4.4913e-02, -3.6372e-04,  9.9899e-01,  4.2108e-02],\n",
      "         [ 6.0530e-03,  9.9998e-01,  9.1950e-05, -3.2148e-02],\n",
      "         [-9.9897e-01,  6.0428e-03,  4.4914e-02,  4.6554e-02],\n",
      "         [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  1.0000e+00]]],\n",
      "       device='cuda:0')\n",
      "estimatedGoalPos6D tensor([[ 0.1000,  0.0000,  0.0000, -0.9000],\n",
      "        [ 0.0000,  0.1000,  0.0000,  0.1000],\n",
      "        [ 0.0000,  0.0000,  0.1000,  0.2000],\n",
      "        [ 0.0000,  0.0000,  0.0000,  1.0000]], device='cuda:0')\n",
      "action_history [2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2]\n",
      "cur_time 2.9000000000000012\n",
      "steps 29\n",
      "most recent position: [[[ 4.4912770e-02 -3.6372221e-04  9.9899089e-01  4.2108230e-02]\n",
      "  [ 6.0530356e-03  9.9998176e-01  9.1949856e-05 -3.2148406e-02]\n",
      "  [-9.9897265e-01  6.0427971e-03  4.4914171e-02  4.6553694e-02]\n",
      "  [ 0.0000000e+00  0.0000000e+00  0.0000000e+00  1.0000000e+00]]]\n",
      "num obstacles: tensor(971477., device='cuda:0')\n",
      "class observation from goal chair\n",
      "ostensible observation from goal tensor([1., 3.], device='cuda:0')\n",
      "pose6D tensor([[[ 2.1857e-01, -1.1381e-03,  9.7582e-01,  4.2362e-02],\n",
      "         [ 4.3666e-03,  9.9999e-01,  1.8818e-04, -3.4800e-02],\n",
      "         [-9.7581e-01,  4.2199e-03,  2.1858e-01,  4.6405e-02],\n",
      "         [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  1.0000e+00]]],\n",
      "       device='cuda:0')\n",
      "estimatedGoalPos6D tensor([[ 0.1000,  0.0000,  0.0000, -0.9000],\n",
      "        [ 0.0000,  0.1000,  0.0000,  0.1000],\n",
      "        [ 0.0000,  0.0000,  0.1000,  0.0000],\n",
      "        [ 0.0000,  0.0000,  0.0000,  1.0000]], device='cuda:0')\n",
      "action_history [2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2]\n",
      "cur_time 3.0000000000000013\n",
      "steps 30\n",
      "most recent position: [[[ 2.1857297e-01 -1.1380692e-03  9.7582000e-01  4.2362258e-02]\n",
      "  [ 4.3666158e-03  9.9999046e-01  1.8818435e-04 -3.4799732e-02]\n",
      "  [-9.7581089e-01  4.2198990e-03  2.1857587e-01  4.6404857e-02]\n",
      "  [ 0.0000000e+00  0.0000000e+00  0.0000000e+00  1.0000000e+00]]]\n",
      "num obstacles: tensor(1005189., device='cuda:0')\n",
      "class observation from goal chair\n",
      "ostensible observation from goal tensor([1., 3.], device='cuda:0')\n",
      "pose6D tensor([[[ 0.3828, -0.0040,  0.9238,  0.0381],\n",
      "         [ 0.0036,  1.0000,  0.0028, -0.0405],\n",
      "         [-0.9238,  0.0023,  0.3828,  0.0487],\n",
      "         [ 0.0000,  0.0000,  0.0000,  1.0000]]], device='cuda:0')\n",
      "estimatedGoalPos6D tensor([[ 0.1000,  0.0000,  0.0000, -0.9000],\n",
      "        [ 0.0000,  0.1000,  0.0000,  0.1000],\n",
      "        [ 0.0000,  0.0000,  0.1000, -0.1000],\n",
      "        [ 0.0000,  0.0000,  0.0000,  1.0000]], device='cuda:0')\n",
      "action_history [2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2]\n",
      "cur_time 3.1000000000000014\n",
      "steps 31\n",
      "most recent position: [[[ 0.38281918 -0.00400284  0.9238147   0.03805219]\n",
      "  [ 0.0036448   0.99998945  0.00282254 -0.04045606]\n",
      "  [-0.9238162   0.00228659  0.3828297   0.04865977]\n",
      "  [ 0.          0.          0.          1.        ]]]\n",
      "num obstacles: tensor(1054318., device='cuda:0')\n",
      "class observation from goal chair\n",
      "ostensible observation from goal tensor([1., 3.], device='cuda:0')\n",
      "pose6D tensor([[[ 5.3766e-01, -3.3159e-03,  8.4316e-01,  3.5749e-02],\n",
      "         [ 2.2642e-03,  9.9999e-01,  2.4889e-03, -4.1104e-02],\n",
      "         [-8.4316e-01,  5.7089e-04,  5.3766e-01,  4.5794e-02],\n",
      "         [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  1.0000e+00]]],\n",
      "       device='cuda:0')\n",
      "estimatedGoalPos6D tensor([[ 0.1000,  0.0000,  0.0000, -0.8000],\n",
      "        [ 0.0000,  0.1000,  0.0000,  0.1000],\n",
      "        [ 0.0000,  0.0000,  0.1000, -0.3000],\n",
      "        [ 0.0000,  0.0000,  0.0000,  1.0000]], device='cuda:0')\n",
      "action_history [2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2]\n",
      "cur_time 3.2000000000000015\n",
      "steps 32\n",
      "most recent position: [[[ 5.3765696e-01 -3.3159151e-03  8.4315717e-01  3.5749402e-02]\n",
      "  [ 2.2641914e-03  9.9999440e-01  2.4889056e-03 -4.1103948e-02]\n",
      "  [-8.4316069e-01  5.7089247e-04  5.3766137e-01  4.5793965e-02]\n",
      "  [ 0.0000000e+00  0.0000000e+00  0.0000000e+00  1.0000000e+00]]]\n",
      "num obstacles: tensor(1105500., device='cuda:0')\n",
      "class observation from goal chair\n",
      "ostensible observation from goal tensor([1., 3.], device='cuda:0')\n",
      "pose6D tensor([[[ 6.7693e-01, -8.3167e-04,  7.3605e-01,  3.9996e-02],\n",
      "         [ 2.0421e-03,  1.0000e+00, -7.4820e-04, -3.5370e-02],\n",
      "         [-7.3605e-01,  2.0096e-03,  6.7693e-01,  4.2672e-02],\n",
      "         [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  1.0000e+00]]],\n",
      "       device='cuda:0')\n",
      "estimatedGoalPos6D tensor([[ 0.1000,  0.0000,  0.0000, -0.7000],\n",
      "        [ 0.0000,  0.1000,  0.0000,  0.1000],\n",
      "        [ 0.0000,  0.0000,  0.1000, -0.5000],\n",
      "        [ 0.0000,  0.0000,  0.0000,  1.0000]], device='cuda:0')\n",
      "action_history [2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2]\n",
      "cur_time 3.3000000000000016\n",
      "steps 33\n",
      "most recent position: [[[ 6.7692834e-01 -8.3167199e-04  7.3604852e-01  3.9996315e-02]\n",
      "  [ 2.0421362e-03  9.9999762e-01 -7.4819766e-04 -3.5370167e-02]\n",
      "  [-7.3604620e-01  2.0095874e-03  6.7692840e-01  4.2672474e-02]\n",
      "  [ 0.0000000e+00  0.0000000e+00  0.0000000e+00  1.0000000e+00]]]\n",
      "num obstacles: tensor(1131956., device='cuda:0')\n",
      "class observation from goal chair\n",
      "ostensible observation from goal tensor([1., 3.], device='cuda:0')\n",
      "pose6D tensor([[[ 0.7898, -0.0027,  0.6134,  0.0264],\n",
      "         [ 0.0042,  1.0000, -0.0011, -0.0352],\n",
      "         [-0.6134,  0.0034,  0.7898,  0.0518],\n",
      "         [ 0.0000,  0.0000,  0.0000,  1.0000]]], device='cuda:0')\n",
      "estimatedGoalPos6D tensor([[ 0.1000,  0.0000,  0.0000, -0.6000],\n",
      "        [ 0.0000,  0.1000,  0.0000,  0.1000],\n",
      "        [ 0.0000,  0.0000,  0.1000, -0.6000],\n",
      "        [ 0.0000,  0.0000,  0.0000,  1.0000]], device='cuda:0')\n",
      "action_history [2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2]\n",
      "cur_time 3.4000000000000017\n",
      "steps 34\n",
      "most recent position: [[[ 0.7897679  -0.00268367  0.6133999   0.02639684]\n",
      "  [ 0.0042292   0.99999046 -0.00107017 -0.03518813]\n",
      "  [-0.61339116  0.00343938  0.78977185  0.05181785]\n",
      "  [ 0.          0.          0.          1.        ]]]\n",
      "num obstacles: tensor(1159393., device='cuda:0')\n",
      "class observation from goal chair\n",
      "ostensible observation from goal tensor([1., 3.], device='cuda:0')\n",
      "pose6D tensor([[[ 8.8266e-01,  8.7117e-04,  4.7001e-01,  4.0571e-02],\n",
      "         [ 2.3959e-03,  9.9998e-01, -6.3529e-03, -3.0650e-02],\n",
      "         [-4.7001e-01,  6.7336e-03,  8.8264e-01,  6.5893e-02],\n",
      "         [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  1.0000e+00]]],\n",
      "       device='cuda:0')\n",
      "estimatedGoalPos6D tensor([[ 0.1000,  0.0000,  0.0000, -0.5000],\n",
      "        [ 0.0000,  0.1000,  0.0000,  0.1000],\n",
      "        [ 0.0000,  0.0000,  0.1000, -0.7000],\n",
      "        [ 0.0000,  0.0000,  0.0000,  1.0000]], device='cuda:0')\n",
      "action_history [2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2]\n",
      "cur_time 3.5000000000000018\n",
      "steps 35\n",
      "most recent position: [[[ 8.8265973e-01  8.7116944e-04  4.7001162e-01  4.0570978e-02]\n",
      "  [ 2.3959214e-03  9.9997693e-01 -6.3528921e-03 -3.0650077e-02]\n",
      "  [-4.7000629e-01  6.7335535e-03  8.8263738e-01  6.5892532e-02]\n",
      "  [ 0.0000000e+00  0.0000000e+00  0.0000000e+00  1.0000000e+00]]]\n",
      "num obstacles: tensor(1183637., device='cuda:0')\n",
      "class observation from goal chair\n",
      "ostensible observation from goal tensor([1., 3.], device='cuda:0')\n",
      "pose6D tensor([[[ 0.9521,  0.0016,  0.3059,  0.0476],\n",
      "         [ 0.0011,  1.0000, -0.0085, -0.0272],\n",
      "         [-0.3059,  0.0084,  0.9520,  0.0602],\n",
      "         [ 0.0000,  0.0000,  0.0000,  1.0000]]], device='cuda:0')\n",
      "estimatedGoalPos6D tensor([[ 0.1000,  0.0000,  0.0000, -0.3000],\n",
      "        [ 0.0000,  0.1000,  0.0000,  0.1000],\n",
      "        [ 0.0000,  0.0000,  0.1000, -0.8000],\n",
      "        [ 0.0000,  0.0000,  0.0000,  1.0000]], device='cuda:0')\n",
      "action_history [2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2]\n",
      "cur_time 3.600000000000002\n",
      "steps 36\n",
      "most recent position: [[[ 0.95207727  0.00155651  0.30585378  0.04764324]\n",
      "  [ 0.00108692  0.9999635  -0.00847229 -0.02723111]\n",
      "  [-0.30585575  0.00839871  0.9520409   0.06016253]\n",
      "  [ 0.          0.          0.          1.        ]]]\n",
      "num obstacles: tensor(1211868., device='cuda:0')\n",
      "class observation from goal chair\n",
      "ostensible observation from goal tensor([1., 3.], device='cuda:0')\n",
      "pose6D tensor([[[ 9.8824e-01,  4.9716e-04,  1.5288e-01,  2.6668e-02],\n",
      "         [ 5.9872e-04,  9.9997e-01, -7.1220e-03, -2.9716e-02],\n",
      "         [-1.5288e-01,  7.1298e-03,  9.8822e-01,  7.3567e-02],\n",
      "         [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  1.0000e+00]]],\n",
      "       device='cuda:0')\n",
      "estimatedGoalPos6D tensor([[ 0.1000,  0.0000,  0.0000, -0.2000],\n",
      "        [ 0.0000,  0.1000,  0.0000,  0.1000],\n",
      "        [ 0.0000,  0.0000,  0.1000, -0.8000],\n",
      "        [ 0.0000,  0.0000,  0.0000,  1.0000]], device='cuda:0')\n",
      "action_history [2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2]\n",
      "cur_time 3.700000000000002\n",
      "steps 37\n",
      "most recent position: [[[ 9.8824435e-01  4.9715652e-04  1.5288150e-01  2.6668277e-02]\n",
      "  [ 5.9871981e-04  9.9997431e-01 -7.1220230e-03 -2.9716320e-02]\n",
      "  [-1.5288115e-01  7.1298326e-03  9.8821884e-01  7.3567197e-02]\n",
      "  [ 0.0000000e+00  0.0000000e+00  0.0000000e+00  1.0000000e+00]]]\n",
      "num obstacles: tensor(1242721., device='cuda:0')\n",
      "class observation from goal chair\n",
      "ostensible observation from goal tensor([1., 3.], device='cuda:0')\n",
      "pose6D tensor([[[ 9.9982e-01, -7.0955e-04, -1.9046e-02,  2.4304e-02],\n",
      "         [ 6.0890e-04,  9.9999e-01, -5.2897e-03, -3.2608e-02],\n",
      "         [ 1.9050e-02,  5.2771e-03,  9.9980e-01,  7.0369e-02],\n",
      "         [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  1.0000e+00]]],\n",
      "       device='cuda:0')\n",
      "estimatedGoalPos6D tensor([[ 0.1000,  0.0000,  0.0000,  0.0000],\n",
      "        [ 0.0000,  0.1000,  0.0000,  0.1000],\n",
      "        [ 0.0000,  0.0000,  0.1000, -0.9000],\n",
      "        [ 0.0000,  0.0000,  0.0000,  1.0000]], device='cuda:0')\n",
      "action_history [2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2]\n",
      "cur_time 3.800000000000002\n",
      "steps 38\n",
      "most recent position: [[[ 9.9981833e-01 -7.0954900e-04 -1.9046485e-02  2.4303814e-02]\n",
      "  [ 6.0889986e-04  9.9998575e-01 -5.2896515e-03 -3.2608416e-02]\n",
      "  [ 1.9050002e-02  5.2770926e-03  9.9980468e-01  7.0369340e-02]\n",
      "  [ 0.0000000e+00  0.0000000e+00  0.0000000e+00  1.0000000e+00]]]\n",
      "num obstacles: tensor(1271610., device='cuda:0')\n",
      "class observation from goal chair\n",
      "ostensible observation from goal tensor([1., 3.], device='cuda:0')\n",
      "pose6D tensor([[[ 9.9102e-01,  4.7179e-04, -1.3372e-01, -2.7932e-02],\n",
      "         [-2.5929e-03,  9.9987e-01, -1.5688e-02, -1.7471e-02],\n",
      "         [ 1.3370e-01,  1.5894e-02,  9.9089e-01,  6.9799e-02],\n",
      "         [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  1.0000e+00]]],\n",
      "       device='cuda:0')\n",
      "estimatedGoalPos6D tensor([[ 0.1000,  0.0000,  0.0000,  0.0000],\n",
      "        [ 0.0000,  0.1000,  0.0000,  0.1000],\n",
      "        [ 0.0000,  0.0000,  0.1000, -0.9000],\n",
      "        [ 0.0000,  0.0000,  0.0000,  1.0000]], device='cuda:0')\n",
      "action_history [2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2]\n",
      "cur_time 3.900000000000002\n",
      "steps 39\n",
      "most recent position: [[[ 9.9101883e-01  4.7179410e-04 -1.3372150e-01 -2.7931804e-02]\n",
      "  [-2.5928547e-03  9.9987358e-01 -1.5688077e-02 -1.7470906e-02]\n",
      "  [ 1.3369717e-01  1.5893903e-02  9.9089473e-01  6.9798730e-02]\n",
      "  [ 0.0000000e+00  0.0000000e+00  0.0000000e+00  1.0000000e+00]]]\n",
      "num obstacles: tensor(1312816., device='cuda:0')\n",
      "class observation from goal chair\n",
      "ostensible observation from goal tensor([1., 3.], device='cuda:0')\n",
      "pose6D tensor([[[ 9.3463e-01, -3.3750e-04, -3.5563e-01,  1.4268e-02],\n",
      "         [ 2.4076e-04,  1.0000e+00, -3.1628e-04,  8.3840e-04],\n",
      "         [ 3.5563e-01,  2.0999e-04,  9.3463e-01, -3.6821e-03],\n",
      "         [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  1.0000e+00]]],\n",
      "       device='cuda:0')\n",
      "estimatedGoalPos6D tensor([[ 0.1000,  0.0000,  0.0000,  0.3000],\n",
      "        [ 0.0000,  0.1000,  0.0000,  0.1000],\n",
      "        [ 0.0000,  0.0000,  0.1000, -0.9000],\n",
      "        [ 0.0000,  0.0000,  0.0000,  1.0000]], device='cuda:0')\n",
      "action_history [2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2]\n",
      "cur_time 4.000000000000002\n",
      "steps 40\n",
      "most recent position: [[[ 9.3462849e-01 -3.3749777e-04 -3.5562533e-01  1.4268092e-02]\n",
      "  [ 2.4075767e-04  9.9999988e-01 -3.1628466e-04  8.3839986e-04]\n",
      "  [ 3.5562539e-01  2.0998891e-04  9.3462849e-01 -3.6820616e-03]\n",
      "  [ 0.0000000e+00  0.0000000e+00  0.0000000e+00  1.0000000e+00]]]\n",
      "num obstacles: tensor(1312997., device='cuda:0')\n",
      "class observation from goal chair\n",
      "ostensible observation from goal tensor([1., 3.], device='cuda:0')\n",
      "pose6D tensor([[[ 8.5083e-01,  4.8470e-04, -5.2544e-01,  2.7924e-02],\n",
      "         [-1.4548e-03,  1.0000e+00, -1.4333e-03,  1.6742e-03],\n",
      "         [ 5.2543e-01,  1.9839e-03,  8.5083e-01,  4.4922e-03],\n",
      "         [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  1.0000e+00]]],\n",
      "       device='cuda:0')\n",
      "estimatedGoalPos6D tensor([[ 0.1000,  0.0000,  0.0000,  0.5000],\n",
      "        [ 0.0000,  0.1000,  0.0000,  0.1000],\n",
      "        [ 0.0000,  0.0000,  0.1000, -0.9000],\n",
      "        [ 0.0000,  0.0000,  0.0000,  1.0000]], device='cuda:0')\n",
      "action_history [2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2]\n",
      "cur_time 4.100000000000001\n",
      "steps 41\n",
      "most recent position: [[[ 8.5083342e-01  4.8469607e-04 -5.2543521e-01  2.7924342e-02]\n",
      "  [-1.4548285e-03  9.9999791e-01 -1.4333288e-03  1.6741551e-03]\n",
      "  [ 5.2543342e-01  1.9839420e-03  8.5083234e-01  4.4922223e-03]\n",
      "  [ 0.0000000e+00  0.0000000e+00  0.0000000e+00  1.0000000e+00]]]\n",
      "num obstacles: tensor(1313171., device='cuda:0')\n",
      "class observation from goal chair\n",
      "ostensible observation from goal tensor([1., 3.], device='cuda:0')\n",
      "pose6D tensor([[[ 7.4168e-01,  8.2918e-04, -6.7075e-01,  3.3673e-02],\n",
      "         [-1.8114e-03,  1.0000e+00, -7.6676e-04,  1.1022e-03],\n",
      "         [ 6.7075e-01,  1.7837e-03,  7.4168e-01,  1.1653e-02],\n",
      "         [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  1.0000e+00]]],\n",
      "       device='cuda:0')\n",
      "estimatedGoalPos6D tensor([[ 0.1000,  0.0000,  0.0000,  0.6000],\n",
      "        [ 0.0000,  0.1000,  0.0000,  0.1000],\n",
      "        [ 0.0000,  0.0000,  0.1000, -0.8000],\n",
      "        [ 0.0000,  0.0000,  0.0000,  1.0000]], device='cuda:0')\n",
      "action_history [2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2]\n",
      "cur_time 4.200000000000001\n",
      "steps 42\n",
      "most recent position: [[[ 7.41679549e-01  8.29178898e-04 -6.70753956e-01  3.36728729e-02]\n",
      "  [-1.81141135e-03  9.99998152e-01 -7.66763347e-04  1.10219454e-03]\n",
      "  [ 6.70752108e-01  1.78370369e-03  7.41679609e-01  1.16532575e-02]\n",
      "  [ 0.00000000e+00  0.00000000e+00  0.00000000e+00  1.00000000e+00]]]\n",
      "num obstacles: tensor(1314382., device='cuda:0')\n",
      "class observation from goal chair\n",
      "ostensible observation from goal tensor([1., 3.], device='cuda:0')\n",
      "pose6D tensor([[[ 6.1115e-01,  1.8901e-03, -7.9152e-01,  3.5332e-02],\n",
      "         [-2.7394e-03,  1.0000e+00,  2.7277e-04,  5.0089e-04],\n",
      "         [ 7.9151e-01,  2.0016e-03,  6.1115e-01,  1.7769e-02],\n",
      "         [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  1.0000e+00]]],\n",
      "       device='cuda:0')\n",
      "estimatedGoalPos6D tensor([[ 0.1000,  0.0000,  0.0000,  0.8000],\n",
      "        [ 0.0000,  0.1000,  0.0000,  0.1000],\n",
      "        [ 0.0000,  0.0000,  0.1000, -0.6000],\n",
      "        [ 0.0000,  0.0000,  0.0000,  1.0000]], device='cuda:0')\n",
      "action_history [2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2]\n",
      "cur_time 4.300000000000001\n",
      "steps 43\n",
      "most recent position: [[[ 6.1114550e-01  1.8901014e-03 -7.9151601e-01  3.5332002e-02]\n",
      "  [-2.7394360e-03  9.9999624e-01  2.7276756e-04  5.0088833e-04]\n",
      "  [ 7.9151344e-01  2.0016076e-03  6.1114830e-01  1.7768983e-02]\n",
      "  [ 0.0000000e+00  0.0000000e+00  0.0000000e+00  1.0000000e+00]]]\n",
      "num obstacles: tensor(1315105., device='cuda:0')\n",
      "class observation from goal chair\n",
      "ostensible observation from goal tensor([1., 3.], device='cuda:0')\n",
      "pose6D tensor([[[ 4.6355e-01,  1.7189e-03, -8.8607e-01,  3.7863e-02],\n",
      "         [-2.0082e-03,  1.0000e+00,  8.8926e-04, -3.5134e-04],\n",
      "         [ 8.8607e-01,  1.3672e-03,  4.6355e-01,  1.9252e-02],\n",
      "         [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  1.0000e+00]]],\n",
      "       device='cuda:0')\n",
      "estimatedGoalPos6D tensor([[ 0.1000,  0.0000,  0.0000,  0.9000],\n",
      "        [ 0.0000,  0.1000,  0.0000,  0.1000],\n",
      "        [ 0.0000,  0.0000,  0.1000, -0.5000],\n",
      "        [ 0.0000,  0.0000,  0.0000,  1.0000]], device='cuda:0')\n",
      "action_history [2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2]\n",
      "cur_time 4.4\n",
      "steps 44\n",
      "most recent position: [[[ 4.6354678e-01  1.7188502e-03 -8.8607079e-01  3.7862942e-02]\n",
      "  [-2.0082067e-03  9.9999774e-01  8.8926085e-04 -3.5134226e-04]\n",
      "  [ 8.8607019e-01  1.3671992e-03  4.6354911e-01  1.9251540e-02]\n",
      "  [ 0.0000000e+00  0.0000000e+00  0.0000000e+00  1.0000000e+00]]]\n",
      "num obstacles: tensor(1315398., device='cuda:0')\n",
      "class observation from goal chair\n",
      "ostensible observation from goal tensor([1., 3.], device='cuda:0')\n",
      "pose6D tensor([[[ 3.0200e-01,  1.4409e-03, -9.5331e-01,  3.8266e-02],\n",
      "         [-7.6085e-04,  1.0000e+00,  1.2704e-03, -1.3513e-03],\n",
      "         [ 9.5331e-01,  3.4165e-04,  3.0200e-01,  2.0679e-02],\n",
      "         [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  1.0000e+00]]],\n",
      "       device='cuda:0')\n",
      "estimatedGoalPos6D tensor([[ 0.1000,  0.0000,  0.0000,  1.0000],\n",
      "        [ 0.0000,  0.1000,  0.0000,  0.1000],\n",
      "        [ 0.0000,  0.0000,  0.1000, -0.4000],\n",
      "        [ 0.0000,  0.0000,  0.0000,  1.0000]], device='cuda:0')\n",
      "action_history [2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2]\n",
      "cur_time 4.5\n",
      "steps 45\n",
      "most recent position: [[[ 3.0200103e-01  1.4408773e-03 -9.5330650e-01  3.8266033e-02]\n",
      "  [-7.6084776e-04  9.9999893e-01  1.2704192e-03 -1.3512757e-03]\n",
      "  [ 9.5330739e-01  3.4165298e-04  3.0200171e-01  2.0678777e-02]\n",
      "  [ 0.0000000e+00  0.0000000e+00  0.0000000e+00  1.0000000e+00]]]\n",
      "num obstacles: tensor(1315577., device='cuda:0')\n",
      "class observation from goal chair\n",
      "ostensible observation from goal tensor([1., 3.], device='cuda:0')\n",
      "pose6D tensor([[[ 1.3131e-01,  2.0408e-03, -9.9134e-01,  3.9930e-02],\n",
      "         [-7.2178e-04,  1.0000e+00,  1.9630e-03, -1.6395e-03],\n",
      "         [ 9.9134e-01,  4.5778e-04,  1.3131e-01,  2.3879e-02],\n",
      "         [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  1.0000e+00]]],\n",
      "       device='cuda:0')\n",
      "estimatedGoalPos6D tensor([[ 0.1000,  0.0000,  0.0000,  1.1000],\n",
      "        [ 0.0000,  0.1000,  0.0000,  0.1000],\n",
      "        [ 0.0000,  0.0000,  0.1000, -0.2000],\n",
      "        [ 0.0000,  0.0000,  0.0000,  1.0000]], device='cuda:0')\n",
      "action_history [2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 1]\n",
      "cur_time 4.6\n",
      "steps 46\n",
      "most recent position: [[[ 1.3130540e-01  2.0407683e-03 -9.9133986e-01  3.9930034e-02]\n",
      "  [-7.2178047e-04  9.9999779e-01  1.9629893e-03 -1.6394766e-03]\n",
      "  [ 9.9134171e-01  4.5777863e-04  1.3130662e-01  2.3878520e-02]\n",
      "  [ 0.0000000e+00  0.0000000e+00  0.0000000e+00  1.0000000e+00]]]\n",
      "num obstacles: tensor(1315767., device='cuda:0')\n",
      "class observation from goal chair\n",
      "ostensible observation from goal tensor([1., 3.], device='cuda:0')\n",
      "pose6D tensor([[[ 0.1303,  0.0044, -0.9915, -0.3760],\n",
      "         [-0.0017,  1.0000,  0.0042, -0.0091],\n",
      "         [ 0.9915,  0.0011,  0.1303,  0.0885],\n",
      "         [ 0.0000,  0.0000,  0.0000,  1.0000]]], device='cuda:0')\n",
      "estimatedGoalPos6D tensor([[ 0.1000,  0.0000,  0.0000,  0.6000],\n",
      "        [ 0.0000,  0.1000,  0.0000,  0.1000],\n",
      "        [ 0.0000,  0.0000,  0.1000, -0.1000],\n",
      "        [ 0.0000,  0.0000,  0.0000,  1.0000]], device='cuda:0')\n",
      "action_history [2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 1, 2]\n",
      "cur_time 4.699999999999999\n",
      "steps 47\n",
      "most recent position: [[[ 0.13026637  0.00435468 -0.99146944 -0.37603965]\n",
      "  [-0.00168779  0.9999899   0.00417035 -0.0090906 ]\n",
      "  [ 0.9914776   0.00113013  0.1302724   0.0885483 ]\n",
      "  [ 0.          0.          0.          1.        ]]]\n",
      "num obstacles: tensor(1336063., device='cuda:0')\n",
      "class observation from goal chair\n",
      "ostensible observation from goal tensor([1., 3.], device='cuda:0')\n",
      "pose6D tensor([[[-4.3642e-02,  5.4791e-03, -9.9903e-01, -3.7519e-01],\n",
      "         [-9.5475e-04,  9.9998e-01,  5.5260e-03, -1.3258e-02],\n",
      "         [ 9.9905e-01,  1.1950e-03, -4.3636e-02,  8.7385e-02],\n",
      "         [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  1.0000e+00]]],\n",
      "       device='cuda:0')\n",
      "estimatedGoalPos6D tensor([[0.1000, 0.0000, 0.0000, 0.7000],\n",
      "        [0.0000, 0.1000, 0.0000, 0.1000],\n",
      "        [0.0000, 0.0000, 0.1000, 0.0000],\n",
      "        [0.0000, 0.0000, 0.0000, 1.0000]], device='cuda:0')\n",
      "action_history [2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 1, 2, 2]\n",
      "cur_time 4.799999999999999\n",
      "steps 48\n",
      "most recent position: [[[-4.3642227e-02  5.4790820e-03 -9.9903220e-01 -3.7519243e-01]\n",
      "  [-9.5474842e-04  9.9998426e-01  5.5260104e-03 -1.3258008e-02]\n",
      "  [ 9.9904668e-01  1.1949919e-03 -4.3636303e-02  8.7384768e-02]\n",
      "  [ 0.0000000e+00  0.0000000e+00  0.0000000e+00  1.0000000e+00]]]\n",
      "num obstacles: tensor(1347348., device='cuda:0')\n",
      "class observation from goal chair\n",
      "ostensible observation from goal tensor([1., 3.], device='cuda:0')\n",
      "pose6D tensor([[[-2.1781e-01,  4.3204e-03, -9.7598e-01, -3.6755e-01],\n",
      "         [ 4.9131e-04,  9.9999e-01,  4.3171e-03, -8.8823e-03],\n",
      "         [ 9.7599e-01,  4.6080e-04, -2.1781e-01,  9.2963e-02],\n",
      "         [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  1.0000e+00]]],\n",
      "       device='cuda:0')\n",
      "estimatedGoalPos6D tensor([[0.1000, 0.0000, 0.0000, 0.7000],\n",
      "        [0.0000, 0.1000, 0.0000, 0.1000],\n",
      "        [0.0000, 0.0000, 0.1000, 0.2000],\n",
      "        [0.0000, 0.0000, 0.0000, 1.0000]], device='cuda:0')\n",
      "action_history [2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 1, 2, 2, 2]\n",
      "cur_time 4.899999999999999\n",
      "steps 49\n",
      "most recent position: [[[-2.1781313e-01  4.3204231e-03 -9.7598094e-01 -3.6755255e-01]\n",
      "  [ 4.9131003e-04  9.9999058e-01  4.3170601e-03 -8.8823419e-03]\n",
      "  [ 9.7599041e-01  4.6080319e-04 -2.1781319e-01  9.2962727e-02]\n",
      "  [ 0.0000000e+00  0.0000000e+00  0.0000000e+00  1.0000000e+00]]]\n",
      "num obstacles: tensor(1364080., device='cuda:0')\n",
      "class observation from goal chair\n",
      "ostensible observation from goal tensor([1., 3.], device='cuda:0')\n",
      "pose6D tensor([[[-0.3820,  0.0052, -0.9242, -0.3647],\n",
      "         [ 0.0010,  1.0000,  0.0052, -0.0115],\n",
      "         [ 0.9242,  0.0010, -0.3820,  0.0855],\n",
      "         [ 0.0000,  0.0000,  0.0000,  1.0000]]], device='cuda:0')\n",
      "estimatedGoalPos6D tensor([[0.1000, 0.0000, 0.0000, 0.7000],\n",
      "        [0.0000, 0.1000, 0.0000, 0.1000],\n",
      "        [0.0000, 0.0000, 0.1000, 0.4000],\n",
      "        [0.0000, 0.0000, 0.0000, 1.0000]], device='cuda:0')\n",
      "action_history [2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 1, 2, 2, 2, 1]\n",
      "cur_time 4.999999999999998\n",
      "steps 50\n",
      "most recent position: [[[-0.3819766   0.00519977 -0.92415726 -0.364726  ]\n",
      "  [ 0.00104457  0.9999859   0.00519468 -0.01149698]\n",
      "  [ 0.9241714   0.0010189  -0.38197672  0.08551374]\n",
      "  [ 0.          0.          0.          1.        ]]]\n",
      "num obstacles: tensor(1379624., device='cuda:0')\n",
      "episode over after 50 steps\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'distance_to_goal': 3.252983570098877,\n",
       " 'success': 0.0,\n",
       " 'spl': 0.0,\n",
       " 'softspl': 0.0}"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from collections import defaultdict\n",
    "from typing import Dict, Optional\n",
    "\n",
    "agg_metrics: Dict = defaultdict(float)\n",
    "\n",
    "num_episodes = len(env.episodes)\n",
    "count_episodes = 0\n",
    "max_steps = 50\n",
    "while count_episodes < num_episodes:\n",
    "    agent.reset()\n",
    "    observations = env.reset()\n",
    "    num_steps = 0\n",
    "    while not env.episode_over and num_steps < max_steps:\n",
    "        action = agent.act(observations)\n",
    "        observations = env.step(action)\n",
    "        num_steps += 1\n",
    "    print(\"episode over after {} steps\".format(num_steps))\n",
    "\n",
    "    metrics = env.get_metrics()\n",
    "    for m, v in metrics.items():\n",
    "        agg_metrics[m] += v\n",
    "    count_episodes += 1\n",
    "\n",
    "avg_metrics = {k: v / count_episodes for k, v in agg_metrics.items()}\n",
    "avg_metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_steps = 10\n",
    "\n",
    "action_mapping = {\n",
    "    #0: 'stop',\n",
    "    1: 'move_forward',\n",
    "    2: 'turn left',\n",
    "    #3: 'turn right'\n",
    "}\n",
    "\n",
    "for i in range(len(env.episodes)):\n",
    "    observations = env.reset()\n",
    "    display_sample(observations['rgb'], observations.get('depth'), np.squeeze(observations['depth']))\n",
    "    \n",
    "    count_steps = 0\n",
    "    while count_steps < max_steps:\n",
    "        action = random.choice(list(action_mapping.keys()))\n",
    "        print(action_mapping[action])\n",
    "        observations = env.step(action)\n",
    "        print('rgb mean:', observations['rgb'].mean())\n",
    "        display_sample(observations['rgb'], observations.get('depth'), np.squeeze(observations['depth']))\n",
    "\n",
    "        count_steps += 1\n",
    "        if env.episode_over:\n",
    "            break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[[0., 0., 0.,  ..., 0., 0., 0.],\n",
       "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "          ...,\n",
       "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "          [0., 0., 0.,  ..., 0., 0., 0.]]]], device='cuda:0')"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "agent.map2DObstacles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x7f307ff0cad0>"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "obs_map = agent.map2DObstacles.cpu().numpy()[0,0]\n",
    "map_scale = 40\n",
    "plt.figure()\n",
    "plt.imshow(obs_map[200-map_scale:200+map_scale, 180-map_scale:200+map_scale])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "benchmark = habitat.Benchmark(args.task_config)\n",
    "metrics = benchmark.evaluate(agent)\n",
    "for k, v in metrics.items():\n",
    "    habitat.logger.info(\"{}: {:.3f}\".format(k, v))\n",
    "    print(k, v)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Environment (conda_pytorch_p36)",
   "language": "python",
   "name": "conda_pytorch_p36"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
